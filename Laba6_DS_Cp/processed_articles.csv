title,summary,published,authors,processed_summary
FreeScale: Unleashing the Resolution of Diffusion Models via Tuning-Free Scale Fusion,"Visual diffusion models achieve remarkable progress, yet they are typically
trained at limited resolutions due to the lack of high-resolution data and
constrained computation resources, hampering their ability to generate
high-fidelity images or videos at higher resolutions. Recent efforts have
explored tuning-free strategies to exhibit the untapped potential
higher-resolution visual generation of pre-trained models. However, these
methods are still prone to producing low-quality visual content with repetitive
patterns. The key obstacle lies in the inevitable increase in high-frequency
information when the model generates visual content exceeding its training
resolution, leading to undesirable repetitive patterns deriving from the
accumulated errors. To tackle this challenge, we propose FreeScale, a
tuning-free inference paradigm to enable higher-resolution visual generation
via scale fusion. Specifically, FreeScale processes information from different
receptive scales and then fuses it by extracting desired frequency components.
Extensive experiments validate the superiority of our paradigm in extending the
capabilities of higher-resolution visual generation for both image and video
models. Notably, compared with the previous best-performing method, FreeScale
unlocks the generation of 8k-resolution images for the first time.",2024-12-12 18:59:59+00:00,"['Haonan Qiu', 'Shiwei Zhang', 'Yujie Wei', 'Ruihang Chu', 'Hangjie Yuan', 'Xiang Wang', 'Yingya Zhang', 'Ziwei Liu']",visual diffusion model achieve remarkable progress typically train limited resolution lack high resolution datum constrain computation resource hamper ability generate high fidelity image video high resolution recent effort explore tuning free strategy exhibit untapped potential high resolution visual generation pre train model method prone produce low quality visual content repetitive pattern key obstacle lie inevitable increase high frequency information model generate visual content exceed training resolution lead undesirable repetitive pattern derive accumulate error tackle challenge propose freescale tuning free inference paradigm enable high resolution visual generation scale fusion specifically freescale process information different receptive scale fuse extract desire frequency component extensive experiment validate superiority paradigm extend capability high resolution visual generation image video model notably compare previous well perform method freescale unlock generation resolution image time
Doe-1: Closed-Loop Autonomous Driving with Large World Model,"End-to-end autonomous driving has received increasing attention due to its
potential to learn from large amounts of data. However, most existing methods
are still open-loop and suffer from weak scalability, lack of high-order
interactions, and inefficient decision-making. In this paper, we explore a
closed-loop framework for autonomous driving and propose a large Driving wOrld
modEl (Doe-1) for unified perception, prediction, and planning. We formulate
autonomous driving as a next-token generation problem and use multi-modal
tokens to accomplish different tasks. Specifically, we use free-form texts
(i.e., scene descriptions) for perception and generate future predictions
directly in the RGB space with image tokens. For planning, we employ a
position-aware tokenizer to effectively encode action into discrete tokens. We
train a multi-modal transformer to autoregressively generate perception,
prediction, and planning tokens in an end-to-end and unified manner.
Experiments on the widely used nuScenes dataset demonstrate the effectiveness
of Doe-1 in various tasks including visual question-answering,
action-conditioned video generation, and motion planning. Code:
https://github.com/wzzheng/Doe.",2024-12-12 18:59:59+00:00,"['Wenzhao Zheng', 'Zetian Xia', 'Yuanhui Huang', 'Sicheng Zuo', 'Jie Zhou', 'Jiwen Lu']",end end autonomous driving receive increase attention potential learn large amount datum exist method open loop suffer weak scalability lack high order interaction inefficient decision making paper explore closed loop framework autonomous driving propose large driving world model unified perception prediction planning formulate autonomous driving token generation problem use multi modal token accomplish different task specifically use free form text scene description perception generate future prediction directly rgb space image token planning employ position aware tokenizer effectively encode action discrete token train multi modal transformer autoregressively generate perception prediction plan token end end unified manner experiment widely nuscene dataset demonstrate effectiveness task include visual question answer action condition video generation motion planning code
GenEx: Generating an Explorable World,"Understanding, navigating, and exploring the 3D physical real world has long
been a central challenge in the development of artificial intelligence. In this
work, we take a step toward this goal by introducing GenEx, a system capable of
planning complex embodied world exploration, guided by its generative
imagination that forms priors (expectations) about the surrounding
environments. GenEx generates an entire 3D-consistent imaginative environment
from as little as a single RGB image, bringing it to life through panoramic
video streams. Leveraging scalable 3D world data curated from Unreal Engine,
our generative model is rounded in the physical world. It captures a continuous
360-degree environment with little effort, offering a boundless landscape for
AI agents to explore and interact with. GenEx achieves high-quality world
generation, robust loop consistency over long trajectories, and demonstrates
strong 3D capabilities such as consistency and active 3D mapping. Powered by
generative imagination of the world, GPT-assisted agents are equipped to
perform complex embodied tasks, including both goal-agnostic exploration and
goal-driven navigation. These agents utilize predictive expectation regarding
unseen parts of the physical world to refine their beliefs, simulate different
outcomes based on potential decisions, and make more informed choices. In
summary, we demonstrate that GenEx provides a transformative platform for
advancing embodied AI in imaginative spaces and brings potential for extending
these capabilities to real-world exploration.",2024-12-12 18:59:57+00:00,"['Taiming Lu', 'Tianmin Shu', 'Junfei Xiao', 'Luoxin Ye', 'Jiahao Wang', 'Cheng Peng', 'Chen Wei', 'Daniel Khashabi', 'Rama Chellappa', 'Alan Yuille', 'Jieneng Chen']",understanding navigating explore physical real world long central challenge development artificial intelligence work step goal introduce genex system capable planning complex embody world exploration guide generative imagination form prior expectation surround environment genex generate entire consistent imaginative environment little single rgb image bring life panoramic video stream leverage scalable world datum curate unreal engine generative model round physical world capture continuous degree environment little effort offer boundless landscape ai agent explore interact genex achieve high quality world generation robust loop consistency long trajectory demonstrate strong capability consistency active mapping power generative imagination world gpt assist agent equip perform complex embodied task include goal agnostic exploration goal drive navigation agent utilize predictive expectation unseen part physical world refine belief simulate different outcome base potential decision informed choice summary demonstrate genex provide transformative platform advance embody ai imaginative space bring potential extend capability real world exploration
OmniDrag: Enabling Motion Control for Omnidirectional Image-to-Video Generation,"As virtual reality gains popularity, the demand for controllable creation of
immersive and dynamic omnidirectional videos (ODVs) is increasing. While
previous text-to-ODV generation methods achieve impressive results, they
struggle with content inaccuracies and inconsistencies due to reliance solely
on textual inputs. Although recent motion control techniques provide
fine-grained control for video generation, directly applying these methods to
ODVs often results in spatial distortion and unsatisfactory performance,
especially with complex spherical motions. To tackle these challenges, we
propose OmniDrag, the first approach enabling both scene- and object-level
motion control for accurate, high-quality omnidirectional image-to-video
generation. Building on pretrained video diffusion models, we introduce an
omnidirectional control module, which is jointly fine-tuned with temporal
attention layers to effectively handle complex spherical motion. In addition,
we develop a novel spherical motion estimator that accurately extracts
motion-control signals and allows users to perform drag-style ODV generation by
simply drawing handle and target points. We also present a new dataset, named
Move360, addressing the scarcity of ODV data with large scene and object
motions. Experiments demonstrate the significant superiority of OmniDrag in
achieving holistic scene-level and fine-grained object-level control for ODV
generation. The project page is available at
https://lwq20020127.github.io/OmniDrag.",2024-12-12 18:59:56+00:00,"['Weiqi Li', 'Shijie Zhao', 'Chong Mou', 'Xuhan Sheng', 'Zhenyu Zhang', 'Qian Wang', 'Junlin Li', 'Li Zhang', 'Jian Zhang']",virtual reality gain popularity demand controllable creation immersive dynamic omnidirectional video odvs increase previous text odv generation method achieve impressive result struggle content inaccuracy inconsistency reliance solely textual input recent motion control technique provide fine grain control video generation directly apply method odvs result spatial distortion unsatisfactory performance especially complex spherical motion tackle challenge propose omnidrag approach enable object level motion control accurate high quality omnidirectional image video generation build pretraine video diffusion model introduce omnidirectional control module jointly fine tune temporal attention layer effectively handle complex spherical motion addition develop novel spherical motion estimator accurately extract motion control signal allow user perform drag style odv generation simply draw handle target point present new dataset name address scarcity odv datum large scene object motion experiment demonstrate significant superiority omnidrag achieve holistic scene level fine grain object level control odv generation project page available
Learning Camera Movement Control from Real-World Drone Videos,"This study seeks to automate camera movement control for filming existing
subjects into attractive videos, contrasting with the creation of non-existent
content by directly generating the pixels. We select drone videos as our test
case due to their rich and challenging motion patterns, distinctive viewing
angles, and precise controls. Existing AI videography methods struggle with
limited appearance diversity in simulation training, high costs of recording
expert operations, and difficulties in designing heuristic-based goals to cover
all scenarios. To avoid these issues, we propose a scalable method that
involves collecting real-world training data to improve diversity, extracting
camera trajectories automatically to minimize annotation costs, and training an
effective architecture that does not rely on heuristics. Specifically, we
collect 99k high-quality trajectories by running 3D reconstruction on online
videos, connecting camera poses from consecutive frames to formulate 3D camera
paths, and using Kalman filter to identify and remove low-quality data.
Moreover, we introduce DVGFormer, an auto-regressive transformer that leverages
the camera path and images from all past frames to predict camera movement in
the next frame. We evaluate our system across 38 synthetic natural scenes and 7
real city 3D scans. We show that our system effectively learns to perform
challenging camera movements such as navigating through obstacles, maintaining
low altitude to increase perceived speed, and orbiting towers and buildings,
which are very useful for recording high-quality videos. Data and code are
available at dvgformer.github.io.",2024-12-12 18:59:54+00:00,"['Yunzhong Hou', 'Liang Zheng', 'Philip Torr']",study seek automate camera movement control film exist subject attractive video contrast creation non existent content directly generate pixel select drone video test case rich challenging motion pattern distinctive viewing angle precise control exist ai videography method struggle limited appearance diversity simulation training high cost recording expert operation difficulty design heuristic base goal cover scenario avoid issue propose scalable method involve collect real world training datum improve diversity extract camera trajectory automatically minimize annotation cost train effective architecture rely heuristic specifically collect high quality trajectory run reconstruction online video connect camera pose consecutive frame formulate camera path kalman filter identify remove low quality datum introduce dvgformer auto regressive transformer leverage camera path image past frame predict camera movement frame evaluate system synthetic natural scene real city scan system effectively learn perform challenge camera movement navigate obstacle maintain low altitude increase perceive speed orbit tower building useful record high quality video datum code available
Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos,"Learning to understand dynamic 3D scenes from imagery is crucial for
applications ranging from robotics to scene reconstruction. Yet, unlike other
problems where large-scale supervised training has enabled rapid progress,
directly supervising methods for recovering 3D motion remains challenging due
to the fundamental difficulty of obtaining ground truth annotations. We present
a system for mining high-quality 4D reconstructions from internet stereoscopic,
wide-angle videos. Our system fuses and filters the outputs of camera pose
estimation, stereo depth estimation, and temporal tracking methods into
high-quality dynamic 3D reconstructions. We use this method to generate
large-scale data in the form of world-consistent, pseudo-metric 3D point clouds
with long-term motion trajectories. We demonstrate the utility of this data by
training a variant of DUSt3R to predict structure and 3D motion from real-world
image pairs, showing that training on our reconstructed data enables
generalization to diverse real-world scenes. Project page:
https://stereo4d.github.io",2024-12-12 18:59:54+00:00,"['Linyi Jin', 'Richard Tucker', 'Zhengqi Li', 'David Fouhey', 'Noah Snavely', 'Aleksander Holynski']",learn understand dynamic scene imagery crucial application range robotic scene reconstruction unlike problem large scale supervise training enable rapid progress directly supervise method recover motion remain challenge fundamental difficulty obtain ground truth annotation present system mine high quality reconstruction internet stereoscopic wide angle video system fuse filter output camera pose estimation stereo depth estimation temporal tracking method high quality dynamic reconstruction use method generate large scale datum form world consistent pseudo metric point cloud long term motion trajectory demonstrate utility datum train variant predict structure motion real world image pair show training reconstructed datum enable generalization diverse real world scene project page
EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via Multimodal LLM,"Significant achievements in personalization of diffusion models have been
witnessed. Conventional tuning-free methods mostly encode multiple reference
images by averaging their image embeddings as the injection condition, but such
an image-independent operation cannot perform interaction among images to
capture consistent visual elements within multiple references. Although the
tuning-based Low-Rank Adaptation (LoRA) can effectively extract consistent
elements within multiple images through the training process, it necessitates
specific finetuning for each distinct image group. This paper introduces
EasyRef, a novel plug-and-play adaptation method that enables diffusion models
to be conditioned on multiple reference images and the text prompt. To
effectively exploit consistent visual elements within multiple images, we
leverage the multi-image comprehension and instruction-following capabilities
of the multimodal large language model (MLLM), prompting it to capture
consistent visual elements based on the instruction. Besides, injecting the
MLLM's representations into the diffusion process through adapters can easily
generalize to unseen domains, mining the consistent visual elements within
unseen data. To mitigate computational costs and enhance fine-grained detail
preservation, we introduce an efficient reference aggregation strategy and a
progressive training scheme. Finally, we introduce MRBench, a new
multi-reference image generation benchmark. Experimental results demonstrate
EasyRef surpasses both tuning-free methods like IP-Adapter and tuning-based
methods like LoRA, achieving superior aesthetic quality and robust zero-shot
generalization across diverse domains.",2024-12-12 18:59:48+00:00,"['Zhuofan Zong', 'Dongzhi Jiang', 'Bingqi Ma', 'Guanglu Song', 'Hao Shao', 'Dazhong Shen', 'Yu Liu', 'Hongsheng Li']",significant achievement personalization diffusion model witness conventional tuning free method encode multiple reference image average image embedding injection condition image independent operation perform interaction image capture consistent visual element multiple reference tuning base low rank adaptation lora effectively extract consistent element multiple image training process necessitate specific finetuning distinct image group paper introduce easyref novel plug play adaptation method enable diffusion model condition multiple reference image text prompt effectively exploit consistent visual element multiple image leverage multi image comprehension instruction follow capability multimodal large language model mllm prompt capture consistent visual element base instruction inject mllm representation diffusion process adapter easily generalize unseen domain mine consistent visual element unseen datum mitigate computational cost enhance fine grain detail preservation introduce efficient reference aggregation strategy progressive training scheme finally introduce mrbench new multi reference image generation benchmark experimental result demonstrate easyref surpass tuning free method like ip adapter tuning base method like lora achieve superior aesthetic quality robust zero shot generalization diverse domain
Context Canvas: Enhancing Text-to-Image Diffusion Models with Knowledge Graph-Based RAG,"We introduce a novel approach to enhance the capabilities of text-to-image
models by incorporating a graph-based RAG. Our system dynamically retrieves
detailed character information and relational data from the knowledge graph,
enabling the generation of visually accurate and contextually rich images. This
capability significantly improves upon the limitations of existing T2I models,
which often struggle with the accurate depiction of complex or culturally
specific subjects due to dataset constraints. Furthermore, we propose a novel
self-correcting mechanism for text-to-image models to ensure consistency and
fidelity in visual outputs, leveraging the rich context from the graph to guide
corrections. Our qualitative and quantitative experiments demonstrate that
Context Canvas significantly enhances the capabilities of popular models such
as Flux, Stable Diffusion, and DALL-E, and improves the functionality of
ControlNet for fine-grained image editing tasks. To our knowledge, Context
Canvas represents the first application of graph-based RAG in enhancing T2I
models, representing a significant advancement for producing high-fidelity,
context-aware multi-faceted images.",2024-12-12 18:59:41+00:00,"['Kavana Venkatesh', 'Yusuf Dalva', 'Ismini Lourentzou', 'Pinar Yanardag']",introduce novel approach enhance capability text image model incorporate graph base rag system dynamically retrieve detailed character information relational datum knowledge graph enable generation visually accurate contextually rich image capability significantly improve limitation exist model struggle accurate depiction complex culturally specific subject dataset constraint furthermore propose novel self correct mechanism text image model ensure consistency fidelity visual output leverage rich context graph guide correction qualitative quantitative experiment demonstrate context canvas significantly enhance capability popular model flux stable diffusion dall e improve functionality controlnet fine grain image editing task knowledge context canvas represent application graph base rag enhance model represent significant advancement produce high fidelity context aware multi faceted image
FluxSpace: Disentangled Semantic Editing in Rectified Flow Transformers,"Rectified flow models have emerged as a dominant approach in image
generation, showcasing impressive capabilities in high-quality image synthesis.
However, despite their effectiveness in visual generation, rectified flow
models often struggle with disentangled editing of images. This limitation
prevents the ability to perform precise, attribute-specific modifications
without affecting unrelated aspects of the image. In this paper, we introduce
FluxSpace, a domain-agnostic image editing method leveraging a representation
space with the ability to control the semantics of images generated by
rectified flow transformers, such as Flux. By leveraging the representations
learned by the transformer blocks within the rectified flow models, we propose
a set of semantically interpretable representations that enable a wide range of
image editing tasks, from fine-grained image editing to artistic creation. This
work offers a scalable and effective image editing approach, along with its
disentanglement capabilities.",2024-12-12 18:59:40+00:00,"['Yusuf Dalva', 'Kavana Venkatesh', 'Pinar Yanardag']",rectified flow model emerge dominant approach image generation showcase impressive capability high quality image synthesis despite effectiveness visual generation rectify flow model struggle disentangle editing image limitation prevent ability perform precise attribute specific modification affect unrelated aspect image paper introduce fluxspace domain agnostic image editing method leverage representation space ability control semantic image generate rectify flow transformer flux leverage representation learn transformer block rectify flow model propose set semantically interpretable representation enable wide range image editing task fine grain image editing artistic creation work offer scalable effective image editing approach disentanglement capability
The S-matrix bootstrap with neural optimizers I: zero double discontinuity,"In this work, we develop machine learning techniques to study nonperturbative
scattering amplitudes. We focus on the two-to-two scattering amplitude of
identical scalar particles, setting the double discontinuity to zero as a
simplifying assumption. Neural networks provide an efficient parameterization
for scattering amplitudes, offering a flexible toolkit to describe their fine
nonperturbative structure. Combined with the bootstrap approach based on the
dispersive representation of the amplitude and machine learning's gradient
descent algorithms, they offer a new method to explore the space of consistent
S-matrices. We derive bounds on the values of the first two low-energy Taylor
coefficients of the amplitude and characterize the resulting amplitudes that
populate the allowed region. Crucially, we parallel our neural network analysis
with the standard S-matrix bootstrap, both primal and dual, and observe perfect
agreement across all approaches.",2024-12-12 18:59:37+00:00,"['Mehmet Asim Gumus', 'Damien Leflot', 'Piotr Tourkine', 'Alexander Zhiboedov']",work develop machine learn technique study nonperturbative scattering amplitude focus scatter amplitude identical scalar particle set double discontinuity zero simplifying assumption neural network provide efficient parameterization scatter amplitude offer flexible toolkit describe fine nonperturbative structure combine bootstrap approach base dispersive representation amplitude machine learning gradient descent algorithm offer new method explore space consistent s matrix derive bound value low energy taylor coefficient amplitude characterize result amplitude populate allow region crucially parallel neural network analysis standard s matrix bootstrap primal dual observe perfect agreement approach
Representing Long Volumetric Video with Temporal Gaussian Hierarchy,"This paper aims to address the challenge of reconstructing long volumetric
videos from multi-view RGB videos. Recent dynamic view synthesis methods
leverage powerful 4D representations, like feature grids or point cloud
sequences, to achieve high-quality rendering results. However, they are
typically limited to short (1~2s) video clips and often suffer from large
memory footprints when dealing with longer videos. To solve this issue, we
propose a novel 4D representation, named Temporal Gaussian Hierarchy, to
compactly model long volumetric videos. Our key observation is that there are
generally various degrees of temporal redundancy in dynamic scenes, which
consist of areas changing at different speeds. Motivated by this, our approach
builds a multi-level hierarchy of 4D Gaussian primitives, where each level
separately describes scene regions with different degrees of content change,
and adaptively shares Gaussian primitives to represent unchanged scene content
over different temporal segments, thus effectively reducing the number of
Gaussian primitives. In addition, the tree-like structure of the Gaussian
hierarchy allows us to efficiently represent the scene at a particular moment
with a subset of Gaussian primitives, leading to nearly constant GPU memory
usage during the training or rendering regardless of the video length.
Extensive experimental results demonstrate the superiority of our method over
alternative methods in terms of training cost, rendering speed, and storage
usage. To our knowledge, this work is the first approach capable of efficiently
handling minutes of volumetric video data while maintaining state-of-the-art
rendering quality. Our project page is available at:
https://zju3dv.github.io/longvolcap.",2024-12-12 18:59:34+00:00,"['Zhen Xu', 'Yinghao Xu', 'Zhiyuan Yu', 'Sida Peng', 'Jiaming Sun', 'Hujun Bao', 'Xiaowei Zhou']",paper aim address challenge reconstruct long volumetric video multi view rgb video recent dynamic view synthesis method leverage powerful representation like feature grid point cloud sequence achieve high quality rendering result typically limited short video clip suffer large memory footprint deal long video solve issue propose novel representation name temporal gaussian hierarchy compactly model long volumetric video key observation generally degree temporal redundancy dynamic scene consist area change different speed motivate approach build multi level hierarchy gaussian primitive level separately describe scene region different degree content change adaptively share gaussian primitive represent unchanged scene content different temporal segment effectively reduce number gaussian primitive addition tree like structure gaussian hierarchy allow efficiently represent scene particular moment subset gaussian primitive lead nearly constant gpu memory usage training rendering regardless video length extensive experimental result demonstrate superiority method alternative method term training cost render speed storage usage knowledge work approach capable efficiently handle minute volumetric video datum maintain state art rendering quality project page available
Spectral Image Tokenizer,"Image tokenizers map images to sequences of discrete tokens, and are a
crucial component of autoregressive transformer-based image generation. The
tokens are typically associated with spatial locations in the input image,
arranged in raster scan order, which is not ideal for autoregressive modeling.
In this paper, we propose to tokenize the image spectrum instead, obtained from
a discrete wavelet transform (DWT), such that the sequence of tokens represents
the image in a coarse-to-fine fashion. Our tokenizer brings several advantages:
1) it leverages that natural images are more compressible at high frequencies,
2) it can take and reconstruct images of different resolutions without
retraining, 3) it improves the conditioning for next-token prediction --
instead of conditioning on a partial line-by-line reconstruction of the image,
it takes a coarse reconstruction of the full image, 4) it enables partial
decoding where the first few generated tokens can reconstruct a coarse version
of the image, 5) it enables autoregressive models to be used for image
upsampling. We evaluate the tokenizer reconstruction metrics as well as
multiscale image generation, text-guided image upsampling and editing.",2024-12-12 18:59:31+00:00,"['Carlos Esteves', 'Mohammed Suhail', 'Ameesh Makadia']",image tokenizer map image sequence discrete token crucial component autoregressive transformer base image generation token typically associate spatial location input image arrange raster scan order ideal autoregressive modeling paper propose tokenize image spectrum instead obtain discrete wavelet transform dwt sequence tokens represent image coarse fine fashion tokenizer bring advantage leverage natural image compressible high frequency reconstruct image different resolution retraining improve conditioning token prediction instead condition partial line line reconstruction image take coarse reconstruction image enable partial decode generate token reconstruct coarse version image enable autoregressive model image upsampling evaluate tokenizer reconstruction metric multiscale image generation text guide image upsampling editing
Feat2GS: Probing Visual Foundation Models with Gaussian Splatting,"Given that visual foundation models (VFMs) are trained on extensive datasets
but often limited to 2D images, a natural question arises: how well do they
understand the 3D world? With the differences in architecture and training
protocols (i.e., objectives, proxy tasks), a unified framework to fairly and
comprehensively probe their 3D awareness is urgently needed. Existing works on
3D probing suggest single-view 2.5D estimation (e.g., depth and normal) or
two-view sparse 2D correspondence (e.g., matching and tracking). Unfortunately,
these tasks ignore texture awareness, and require 3D data as ground-truth,
which limits the scale and diversity of their evaluation set. To address these
issues, we introduce Feat2GS, which readout 3D Gaussians attributes from VFM
features extracted from unposed images. This allows us to probe 3D awareness
for geometry and texture via novel view synthesis, without requiring 3D data.
Additionally, the disentanglement of 3DGS parameters - geometry
($\boldsymbol{x}, \alpha, \Sigma$) and texture ($\boldsymbol{c}$) - enables
separate analysis of texture and geometry awareness. Under Feat2GS, we conduct
extensive experiments to probe the 3D awareness of several VFMs, and
investigate the ingredients that lead to a 3D aware VFM. Building on these
findings, we develop several variants that achieve state-of-the-art across
diverse datasets. This makes Feat2GS useful for probing VFMs, and as a
simple-yet-effective baseline for novel-view synthesis. Code and data will be
made available at https://fanegg.github.io/Feat2GS/.",2024-12-12 18:59:28+00:00,"['Yue Chen', 'Xingyu Chen', 'Anpei Chen', 'Gerard Pons-Moll', 'Yuliang Xiu']",give visual foundation model vfm train extensive dataset limit image natural question arise understand world difference architecture training protocol objective proxy task unified framework fairly comprehensively probe awareness urgently need exist work probing suggest single view estimation depth normal view sparse correspondence matching tracking unfortunately task ignore texture awareness require datum ground truth limit scale diversity evaluation set address issue introduce readout gaussians attribute vfm feature extract unposed image allow probe awareness geometry texture novel view synthesis require datum additionally disentanglement parameter geometry texture enable separate analysis texture geometry awareness conduct extensive experiment probe awareness vfm investigate ingredient lead aware vfm build finding develop variant achieve state art diverse dataset make useful probe vfm simple effective baseline novel view synthesis code datum available
AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials,"Graphical User Interface (GUI) agents hold great potential for automating
complex tasks across diverse digital environments, from web applications to
desktop software. However, the development of such agents is hindered by the
lack of high-quality, multi-step trajectory data required for effective
training. Existing approaches rely on expensive and labor-intensive human
annotation, making them unsustainable at scale. To address this challenge, we
propose AgentTrek, a scalable data synthesis pipeline that generates
high-quality GUI agent trajectories by leveraging web tutorials. Our method
automatically gathers tutorial-like texts from the internet, transforms them
into task goals with step-by-step instructions, and employs a visual-language
model agent to simulate their execution in a real digital environment. A
VLM-based evaluator ensures the correctness of the generated trajectories. We
demonstrate that training GUI agents with these synthesized trajectories
significantly improves their grounding and planning performance over the
current models. Moreover, our approach is more cost-efficient compared to
traditional human annotation methods. This work underscores the potential of
guided replay with web tutorials as a viable strategy for large-scale GUI agent
training, paving the way for more capable and autonomous digital agents.",2024-12-12 18:59:27+00:00,"['Yiheng Xu', 'Dunjie Lu', 'Zhennan Shen', 'Junli Wang', 'Zekun Wang', 'Yuchen Mao', 'Caiming Xiong', 'Tao Yu']",graphical user interface gui agent hold great potential automate complex task diverse digital environment web application desktop software development agent hinder lack high quality multi step trajectory datum require effective training exist approach rely expensive labor intensive human annotation make unsustainable scale address challenge propose agenttrek scalable datum synthesis pipeline generate high quality gui agent trajectory leverage web tutorial method automatically gather tutorial like text internet transform task goal step step instruction employ visual language model agent simulate execution real digital environment vlm base evaluator ensure correctness generate trajectory demonstrate training gui agent synthesized trajectory significantly improve grounding planning performance current model approach cost efficient compare traditional human annotation method work underscore potential guide replay web tutorial viable strategy large scale gui agent training pave way capable autonomous digital agent
SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding,"The remarkable success of Large Language Models (LLMs) has extended to the
multimodal domain, achieving outstanding performance in image understanding and
generation. Recent efforts to develop unified Multimodal Large Language Models
(MLLMs) that integrate these capabilities have shown promising results.
However, existing approaches often involve complex designs in model
architecture or training pipeline, increasing the difficulty of model training
and scaling. In this paper, we propose SynerGen-VL, a simple yet powerful
encoder-free MLLM capable of both image understanding and generation. To
address challenges identified in existing encoder-free unified MLLMs, we
introduce the token folding mechanism and the vision-expert-based progressive
alignment pretraining strategy, which effectively support high-resolution image
understanding while reducing training complexity. After being trained on
large-scale mixed image-text data with a unified next-token prediction
objective, SynerGen-VL achieves or surpasses the performance of existing
encoder-free unified MLLMs with comparable or smaller parameter sizes, and
narrows the gap with task-specific state-of-the-art models, highlighting a
promising path toward future unified MLLMs. Our code and models shall be
released.",2024-12-12 18:59:26+00:00,"['Hao Li', 'Changyao Tian', 'Jie Shao', 'Xizhou Zhu', 'Zhaokai Wang', 'Jinguo Zhu', 'Wenhan Dou', 'Xiaogang Wang', 'Hongsheng Li', 'Lewei Lu', 'Jifeng Dai']",remarkable success large language models llms extend multimodal domain achieve outstanding performance image understanding generation recent effort develop unified multimodal large language model mllm integrate capability show promise result exist approach involve complex design model architecture training pipeline increase difficulty model training scaling paper propose synergen vl simple powerful encoder free mllm capable image understanding generation address challenge identify exist encoder free unified mllm introduce token folding mechanism vision expert base progressive alignment pretraining strategy effectively support high resolution image understanding reduce training complexity train large scale mixed image text datum unified token prediction objective synergen vl achieve surpass performance exist encoder free unified mllm comparable small parameter size narrow gap task specific state art model highlight promising path future unify mllm code model shall release
Hidden Biases of End-to-End Driving Datasets,"End-to-end driving systems have made rapid progress, but have so far not been
applied to the challenging new CARLA Leaderboard 2.0. Further, while there is a
large body of literature on end-to-end architectures and training strategies,
the impact of the training dataset is often overlooked. In this work, we make a
first attempt at end-to-end driving for Leaderboard 2.0. Instead of
investigating architectures, we systematically analyze the training dataset,
leading to new insights: (1) Expert style significantly affects downstream
policy performance. (2) In complex data sets, the frames should not be weighted
on the basis of simplistic criteria such as class frequencies. (3) Instead,
estimating whether a frame changes the target labels compared to previous
frames can reduce the size of the dataset without removing important
information. By incorporating these findings, our model ranks first and second
respectively on the map and sensors tracks of the 2024 CARLA Challenge, and
sets a new state-of-the-art on the Bench2Drive test routes. Finally, we uncover
a design flaw in the current evaluation metrics and propose a modification for
future challenges. Our dataset, code, and pre-trained models are publicly
available at https://github.com/autonomousvision/carla_garage.",2024-12-12 18:59:13+00:00,"['Julian Zimmerlin', 'Jens Beißwenger', 'Bernhard Jaeger', 'Andreas Geiger', 'Kashyap Chitta']",end end driving system rapid progress far apply challenging new carla leaderboard large body literature end end architecture training strategy impact training dataset overlook work attempt end end driving leaderboard instead investigate architecture systematically analyze training dataset lead new insight expert style significantly affect downstream policy performance complex data set frame weight basis simplistic criterion class frequency instead estimate frame change target label compare previous frame reduce size dataset remove important information incorporate finding model rank second respectively map sensor track carla challenge set new state art test route finally uncover design flaw current evaluation metric propose modification future challenge dataset code pre train model publicly available
Owl-1: Omni World Model for Consistent Long Video Generation,"Video generation models (VGMs) have received extensive attention recently and
serve as promising candidates for general-purpose large vision models. While
they can only generate short videos each time, existing methods achieve long
video generation by iteratively calling the VGMs, using the last-frame output
as the condition for the next-round generation. However, the last frame only
contains short-term fine-grained information about the scene, resulting in
inconsistency in the long horizon. To address this, we propose an Omni World
modeL (Owl-1) to produce long-term coherent and comprehensive conditions for
consistent long video generation. As videos are observations of the underlying
evolving world, we propose to model the long-term developments in a latent
space and use VGMs to film them into videos. Specifically, we represent the
world with a latent state variable which can be decoded into explicit video
observations. These observations serve as a basis for anticipating temporal
dynamics which in turn update the state variable. The interaction between
evolving dynamics and persistent state enhances the diversity and consistency
of the long videos. Extensive experiments show that Owl-1 achieves comparable
performance with SOTA methods on VBench-I2V and VBench-Long, validating its
ability to generate high-quality video observations. Code:
https://github.com/huang-yh/Owl.",2024-12-12 18:59:01+00:00,"['Yuanhui Huang', 'Wenzhao Zheng', 'Yuan Gao', 'Xin Tao', 'Pengfei Wan', 'Di Zhang', 'Jie Zhou', 'Jiwen Lu']",video generation model vgms receive extensive attention recently serve promising candidate general purpose large vision model generate short video time exist method achieve long video generation iteratively call vgm frame output condition round generation frame contain short term fine grain information scene result inconsistency long horizon address propose omni world model produce long term coherent comprehensive condition consistent long video generation video observation underlie evolve world propose model long term development latent space use vgm film video specifically represent world latent state variable decode explicit video observation observation serve basis anticipate temporal dynamic turn update state variable interaction evolve dynamic persistent state enhance diversity consistency long video extensive experiment achieve comparable performance sota method vbench vbench long validate ability generate high quality video observation code
RatBodyFormer: Rodent Body Surface from Keypoints,"Rat behavior modeling goes to the heart of many scientific studies, yet the
textureless body surface evades automatic analysis as it literally has no
keypoints that detectors can find. The movement of the body surface, however,
is a rich source of information for deciphering the rat behavior. We introduce
two key contributions to automatically recover densely 3D sampled rat body
surface points, passively. The first is RatDome, a novel multi-camera system
for rat behavior capture, and a large-scale dataset captured with it that
consists of pairs of 3D keypoints and 3D body surface points. The second is
RatBodyFormer, a novel network to transform detected keypoints to 3D body
surface points. RatBodyFormer is agnostic to the exact locations of the 3D body
surface points in the training data and is trained with masked-learning. We
experimentally validate our framework with a number of real-world experiments.
Our results collectively serve as a novel foundation for automated rat behavior
analysis and will likely have far-reaching implications for biomedical and
neuroscientific research.",2024-12-12 18:59:00+00:00,"['Ayaka Higami', 'Karin Oshima', 'Tomoyo Isoguchi Shiramatsu', 'Hirokazu Takahashi', 'Shohei Nobuhara', 'Ko Nishino']",rat behavior modeling go heart scientific study textureless body surface evade automatic analysis literally keypoint detector find movement body surface rich source information decipher rat behavior introduce key contribution automatically recover densely sample rat body surface point passively ratdome novel multi camera system rat behavior capture large scale dataset capture consist pair keypoint body surface point second ratbodyformer novel network transform detect keypoint body surface point ratbodyformer agnostic exact location body surface point training datum train mask learning experimentally validate framework number real world experiment result collectively serve novel foundation automated rat behavior analysis likely far reach implication biomedical neuroscientific research
LiftImage3D: Lifting Any Single Image to 3D Gaussians with Video Generation Priors,"Single-image 3D reconstruction remains a fundamental challenge in computer
vision due to inherent geometric ambiguities and limited viewpoint information.
Recent advances in Latent Video Diffusion Models (LVDMs) offer promising 3D
priors learned from large-scale video data. However, leveraging these priors
effectively faces three key challenges: (1) degradation in quality across large
camera motions, (2) difficulties in achieving precise camera control, and (3)
geometric distortions inherent to the diffusion process that damage 3D
consistency. We address these challenges by proposing LiftImage3D, a framework
that effectively releases LVDMs' generative priors while ensuring 3D
consistency. Specifically, we design an articulated trajectory strategy to
generate video frames, which decomposes video sequences with large camera
motions into ones with controllable small motions. Then we use robust neural
matching models, i.e. MASt3R, to calibrate the camera poses of generated frames
and produce corresponding point clouds. Finally, we propose a distortion-aware
3D Gaussian splatting representation, which can learn independent distortions
between frames and output undistorted canonical Gaussians. Extensive
experiments demonstrate that LiftImage3D achieves state-of-the-art performance
on two challenging datasets, i.e. LLFF, DL3DV, and Tanks and Temples, and
generalizes well to diverse in-the-wild images, from cartoon illustrations to
complex real-world scenes.",2024-12-12 18:58:42+00:00,"['Yabo Chen', 'Chen Yang', 'Jiemin Fang', 'Xiaopeng Zhang', 'Lingxi Xie', 'Wei Shen', 'Wenrui Dai', 'Hongkai Xiong', 'Qi Tian']",single image reconstruction remain fundamental challenge computer vision inherent geometric ambiguity limited viewpoint information recent advance latent video diffusion models lvdms offer promise prior learn large scale video datum leverage prior effectively face key challenge degradation quality large camera motion difficulty achieve precise camera control geometric distortion inherent diffusion process damage consistency address challenge propose framework effectively release lvdms generative prior ensure consistency specifically design articulate trajectory strategy generate video frame decompose video sequence large camera motion one controllable small motion use robust neural matching model calibrate camera pose generate frame produce correspond point cloud finally propose distortion aware gaussian splatte representation learn independent distortion frame output undistorted canonical gaussians extensive experiment demonstrate achieve state art performance challenging dataset llff tank temples generalize diverse wild image cartoon illustration complex real world scene
InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions,"Creating AI systems that can interact with environments over long periods,
similar to human cognition, has been a longstanding research goal. Recent
advancements in multimodal large language models (MLLMs) have made significant
strides in open-world understanding. However, the challenge of continuous and
simultaneous streaming perception, memory, and reasoning remains largely
unexplored. Current MLLMs are constrained by their sequence-to-sequence
architecture, which limits their ability to process inputs and generate
responses simultaneously, akin to being unable to think while perceiving.
Furthermore, relying on long contexts to store historical data is impractical
for long-term interactions, as retaining all information becomes costly and
inefficient. Therefore, rather than relying on a single foundation model to
perform all functions, this project draws inspiration from the concept of the
Specialized Generalist AI and introduces disentangled streaming perception,
reasoning, and memory mechanisms, enabling real-time interaction with streaming
video and audio input. The proposed framework InternLM-XComposer2.5-OmniLive
(IXC2.5-OL) consists of three key modules: (1) Streaming Perception Module:
Processes multimodal information in real-time, storing key details in memory
and triggering reasoning in response to user queries. (2) Multi-modal Long
Memory Module: Integrates short-term and long-term memory, compressing
short-term memories into long-term ones for efficient retrieval and improved
accuracy. (3) Reasoning Module: Responds to queries and executes reasoning
tasks, coordinating with the perception and memory modules. This project
simulates human-like cognition, enabling multimodal large language models to
provide continuous and adaptive service over time.",2024-12-12 18:58:30+00:00,"['Pan Zhang', 'Xiaoyi Dong', 'Yuhang Cao', 'Yuhang Zang', 'Rui Qian', 'Xilin Wei', 'Lin Chen', 'Yifei Li', 'Junbo Niu', 'Shuangrui Ding', 'Qipeng Guo', 'Haodong Duan', 'Xin Chen', 'Han Lv', 'Zheng Nie', 'Min Zhang', 'Bin Wang', 'Wenwei Zhang', 'Xinyue Zhang', 'Jiaye Ge', 'Wei Li', 'Jingwen Li', 'Zhongying Tu', 'Conghui He', 'Xingcheng Zhang', 'Kai Chen', 'Yu Qiao', 'Dahua Lin', 'Jiaqi Wang']",create ai system interact environment long period similar human cognition longstanding research goal recent advancement multimodal large language model mllm significant stride open world understanding challenge continuous simultaneous streaming perception memory reasoning remain largely unexplored current mllm constrain sequence sequence architecture limit ability process input generate response simultaneously akin unable think perceive furthermore rely long context store historical datum impractical long term interaction retain information costly inefficient rely single foundation model perform function project draw inspiration concept specialized generalist ai introduce disentangle stream perception reasoning memory mechanism enable real time interaction streaming video audio input propose framework internlm omnilive ol consist key module streaming perception module process multimodal information real time store key detail memory trigger reasoning response user query multi modal long memory module integrate short term long term memory compress short term memory long term one efficient retrieval improve accuracy reasoning module respond query execute reasoning task coordinate perception memory module project simulate human like cognition enable multimodal large language model provide continuous adaptive service time
Probing a diffuse flux of axion-like particles from galactic supernovae with neutrino water Cherenkov detectors,"In this article, we claim that axion-like particles (ALPs) with MeV masses
can be produced with semi-relativistic velocities in core-collapse supernovae
(SNe), generating a diffuse galactic flux. We show that these ALPs can be
detected in neutrino water Cherenkov detectors via $a \, p \rightarrow p \,
\gamma$ interactions. Using Super-Kamiokande data, we derive new constraints on
the ALP parameter space, excluding a region spanning more than one order of
magnitude in the ALP-proton coupling above cooling bounds for ALP masses in the
range of $1-80$ MeV and ALP-proton couplings between
$6\times10^{-6}-2\times10^{-4}$. We show that the future Hyper-Kamiokande will
be able to probe couplings as small as $2\times10^{-6}$, fully closing the
allowed region above SN 1987A cooling bounds.",2024-12-12 18:58:24+00:00,"['David Alonso-González', 'David Cerdeño', 'Marina Cermeño', 'Andres D. Perez']",article claim axion like particle alps mev masse produce semi relativistic velocity core collapse supernovae sne generate diffuse galactic flux alp detect neutrino water cherenkov detector p p interaction super kamiokande datum derive new constraint alp parameter space exclude region span order magnitude alp proton coupling cool bound alp masse range mev alp proton coupling future hyper kamiokande able probe coupling small fully close allow region sn cool bound
Wait-Less Offline Tuning and Re-solving for Online Decision Making,"Online linear programming (OLP) has found broad applications in revenue
management and resource allocation. State-of-the-art OLP algorithms achieve low
regret by repeatedly solving linear programming (LP) subproblems that
incorporate updated resource information. However, LP-based methods are
computationally expensive and often inefficient for large-scale applications.
In contrast, recent first-order OLP algorithms are more computationally
efficient but typically suffer from worse regret guarantees. To address these
shortcomings, we propose a new algorithm that combines the strengths of
LP-based and first-order OLP methods. The algorithm re-solves the LP
subproblems periodically at a predefined frequency $f$ and uses the latest dual
prices to guide online decision-making. In addition, a first-order method runs
in parallel during each interval between LP re-solves, smoothing resource
consumption. Our algorithm achieves $\mathscr{O}(\log (T/f) + \sqrt{f})$
regret, delivering a ""wait-less"" online decision-making process that balances
the computational efficiency of first-order methods and the superior regret
guarantee of LP-based methods.",2024-12-12 18:58:14+00:00,"['Jingruo Sun', 'Wenzhi Gao', 'Ellen Vitercik', 'Yinyu Ye']",online linear programming olp find broad application revenue management resource allocation state art olp algorithm achieve low regret repeatedly solve linear programming lp subproblem incorporate update resource information lp base method computationally expensive inefficient large scale application contrast recent order olp algorithm computationally efficient typically suffer bad regret guarantee address shortcoming propose new algorithm combine strength lp base order olp method algorithm solve lp subproblem periodically predefine frequency use late dual price guide online decision making addition order method run parallel interval lp solve smooth resource consumption algorithm achieve t f regret deliver wait online decision make process balance computational efficiency order method superior regret guarantee lp base method
Dissipative measure-valued solutions and weak-strong uniqueness for the Euler alignment system,"We introduce the concept of a dissipative measure-valued solution to the
Euler alignment system. This approach incorporates a modified total energy
balance, utilizing a binary tensor Young measure. The central finding is a weak
(measure-valued)--strong uniqueness principle: if both a dissipative
measure-valued solution and a classical smooth solution originate from the same
initial data, they will be identical as long as the classical solution exists.",2024-12-12 18:56:14+00:00,"['Abhishek Chaudhary', 'Ujjwal Koley', 'Emil Wiedemann']",introduce concept dissipative measure value solution euler alignment system approach incorporate modify total energy balance utilize binary tensor young measure central finding weak measure uniqueness principle dissipative measure value solution classical smooth solution originate initial datum identical long classical solution exist
Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders,"We address the problem of gaze target estimation, which aims to predict where
a person is looking in a scene. Predicting a person's gaze target requires
reasoning both about the person's appearance and the contents of the scene.
Prior works have developed increasingly complex, hand-crafted pipelines for
gaze target estimation that carefully fuse features from separate scene
encoders, head encoders, and auxiliary models for signals like depth and pose.
Motivated by the success of general-purpose feature extractors on a variety of
visual tasks, we propose Gaze-LLE, a novel transformer framework that
streamlines gaze target estimation by leveraging features from a frozen DINOv2
encoder. We extract a single feature representation for the scene, and apply a
person-specific positional prompt to decode gaze with a lightweight module. We
demonstrate state-of-the-art performance across several gaze benchmarks and
provide extensive analysis to validate our design choices. Our code is
available at: http://github.com/fkryan/gazelle .",2024-12-12 18:55:30+00:00,"['Fiona Ryan', 'Ajay Bati', 'Sangmin Lee', 'Daniel Bolya', 'Judy Hoffman', 'James M. Rehg']",address problem gaze target estimation aim predict person look scene predict person gaze target require reasoning person appearance content scene prior work develop increasingly complex hand craft pipeline gaze target estimation carefully fuse feature separate scene encoder head encoder auxiliary model signal like depth pose motivate success general purpose feature extractor variety visual task propose gaze lle novel transformer framework streamline gaze target estimation leverage feature frozen encoder extract single feature representation scene apply person specific positional prompt decode gaze lightweight module demonstrate state art performance gaze benchmark provide extensive analysis validate design choice code available
BaB-ND: Long-Horizon Motion Planning with Branch-and-Bound and Neural Dynamics,"Neural-network-based dynamics models learned from observational data have
shown strong predictive capabilities for scene dynamics in robotic manipulation
tasks. However, their inherent non-linearity presents significant challenges
for effective planning. Current planning methods, often dependent on extensive
sampling or local gradient descent, struggle with long-horizon motion planning
tasks involving complex contact events. In this paper, we present a
GPU-accelerated branch-and-bound (BaB) framework for motion planning in
manipulation tasks that require trajectory optimization over neural dynamics
models. Our approach employs a specialized branching heuristics to divide the
search space into subdomains, and applies a modified bound propagation method,
inspired by the state-of-the-art neural network verifier alpha-beta-CROWN, to
efficiently estimate objective bounds within these subdomains. The branching
process guides planning effectively, while the bounding process strategically
reduces the search space. Our framework achieves superior planning performance,
generating high-quality state-action trajectories and surpassing existing
methods in challenging, contact-rich manipulation tasks such as non-prehensile
planar pushing with obstacles, object sorting, and rope routing in both
simulated and real-world settings. Furthermore, our framework supports various
neural network architectures, ranging from simple multilayer perceptrons to
advanced graph neural dynamics models, and scales efficiently with different
model sizes.",2024-12-12 18:55:14+00:00,"['Keyi Shen', 'Jiangwei Yu', 'Huan Zhang', 'Yunzhu Li']",neural network base dynamic model learn observational datum show strong predictive capability scene dynamic robotic manipulation task inherent non linearity present significant challenge effective planning current planning method dependent extensive sampling local gradient descent struggle long horizon motion planning task involve complex contact event paper present gpu accelerate branch bind bab framework motion plan manipulation task require trajectory optimization neural dynamic model approach employ specialized branching heuristic divide search space subdomain apply modify bind propagation method inspire state art neural network verifier alpha beta crown efficiently estimate objective bound subdomain branching process guide plan effectively bounding process strategically reduce search space framework achieve superior planning performance generate high quality state action trajectory surpass exist method challenging contact rich manipulation task non prehensile planar push obstacle object sorting rope route simulated real world setting furthermore framework support neural network architecture range simple multilayer perceptron advanced graph neural dynamic model scale efficiently different model size
Gradient-Boosted Mixture Regression Models for Postprocessing Ensemble Weather Forecasts,"Nowadays, weather forecasts are commonly generated by ensemble forecasts
based on multiple runs of numerical weather prediction models. However, such
forecasts are usually miscalibrated and/or biased, thus require statistical
postprocessing. Non-homogeneous regression models, such as the ensemble model
output statistics are frequently applied to correct these forecasts.
Nonetheless, these methods often rely on the assumption of an unimodal
parametric distribution, leading to improved, but sometimes not fully
calibrated forecasts. To address this issue, a mixture regression model is
presented, where the ensemble forecasts of each exchangeable group are linked
to only one mixture component and mixture weight, called mixture of model
output statistics (MIXMOS). In order to remove location specific effects and to
use a longer training data, the standardized anomalies of the response and the
ensemble forecasts are employed for the mixture of standardized anomaly model
output statistics (MIXSAMOS). As carefully selected covariates, e.g. from
different weather variables, can enhance model performance, the non-cyclic
gradient-boosting algorithm for mixture regression models is introduced.
Furthermore, MIXSAMOS is extended by this gradient-boosting algorithm
(MIXSAMOS-GB) providing an automatic variable selection. The novel mixture
regression models substantially outperform state-of-the-art postprocessing
models in a case study for 2m surface temperature forecasts in Germany.",2024-12-12 18:54:56+00:00,['David Jobst'],nowadays weather forecast commonly generate ensemble forecast base multiple run numerical weather prediction model forecast usually miscalibrate biased require statistical postprocessing non homogeneous regression model ensemble model output statistic frequently apply correct forecast nonetheless method rely assumption unimodal parametric distribution lead improve fully calibrate forecast address issue mixture regression model present ensemble forecast exchangeable group link mixture component mixture weight call mixture model output statistic mixmos order remove location specific effect use long training datum standardized anomaly response ensemble forecast employ mixture standardized anomaly model output statistic mixsamos carefully select covariate different weather variable enhance model performance non cyclic gradient boost algorithm mixture regression model introduce furthermore mixsamos extend gradient boost algorithm mixsamo gb provide automatic variable selection novel mixture regression model substantially outperform state art postprocessing model case study m surface temperature forecast germany
Neptune: The Long Orbit to Benchmarking Long Video Understanding,"This paper describes a semi-automatic pipeline to generate challenging
question-answer-decoy sets for understanding long videos. Many existing video
datasets and models are focused on short clips (10s-30s). While some long video
datasets do exist, they can often be solved by powerful image models applied
per frame (and often to very few frames) in a video, and are usually manually
annotated at high cost. In order to mitigate both these problems, we propose a
scalable dataset creation pipeline which leverages large models (VLMs and
LLMs), to automatically generate dense, time-aligned video captions, as well as
tough question answer decoy sets for video segments (up to 15 minutes in
length). Our dataset Neptune covers a broad range of long video reasoning
abilities and consists of a subset that emphasizes multimodal reasoning. Since
existing metrics for open-ended question answering are either rule-based or may
rely on proprietary models, we provide a new open source model-based metric GEM
to score open-ended responses on Neptune. Benchmark evaluations reveal that
most current open-source long video models perform poorly on Neptune,
particularly on questions testing temporal ordering, counting and state
changes. Through Neptune, we aim to spur the development of more advanced
models capable of understanding long videos. The dataset is available at
https://github.com/google-deepmind/neptune",2024-12-12 18:54:48+00:00,"['Arsha Nagrani', 'Mingda Zhang', 'Ramin Mehran', 'Rachel Hornung', 'Nitesh Bharadwaj Gundavarapu', 'Nilpa Jha', 'Austin Myers', 'Xingyi Zhou', 'Boqing Gong', 'Cordelia Schmid', 'Mikhail Sirotenko', 'Yukun Zhu', 'Tobias Weyand']",paper describe semi automatic pipeline generate challenging question answer decoy set understand long video exist video dataset model focus short clip long video dataset exist solve powerful image model apply frame frame video usually manually annotate high cost order mitigate problem propose scalable dataset creation pipeline leverage large model vlm llms automatically generate dense time align video caption tough question answer decoy set video segment minute length dataset neptune cover broad range long video reasoning ability consist subset emphasize multimodal reasoning exist metric open ended question answering rule base rely proprietary model provide new open source model base metric gem score open end response neptune benchmark evaluation reveal current open source long video model perform poorly neptune particularly question test temporal ordering counting state change neptune aim spur development advanced model capable understand long video dataset available
Optical Arbitrary Waveform Generation (OAWG) Using Actively Phase-Stabilized Spectral Stitching,"The conventional way of generating optical waveforms relies on the in-phase
and quadrature (IQ) modulation of a continuous wave (CW) laser tone. In this
case, the bandwidth of the resulting optical waveform is limited by the
underlying electronic components, in particular by the digital-to-analog
converters (DACs) generating the drive signals for the IQ modulator. This
bandwidth bottleneck can be overcome by using a concept known as optical
arbitrary waveform generation (OAWG), where multiple IQ modulators and DACs are
operated in parallel to first synthesize individual spectral slices, which are
subsequently combined to form a single ultra-broadband arbitrary optical
waveform. However, targeted synthesis of arbitrary optical waveforms from
multiple spectral slices has so far been hampered by difficulties to maintain
the correct optical phase relationship between the slices. In this paper, we
propose and demonstrate spectrally sliced OAWG with active phase stabilization,
which permits targeted synthesis of truly arbitrary optical waveforms. We
demonstrate the viability of the scheme by synthesizing optical waveforms with
record-high bandwidths of up to 325 GHz from four individually generated
optical tributaries. In a proof-of-concept experiment, we use the OAWG system
to generate 32QAM data signals at symbol rates of up to 320 GBd, which we
transmit over 87 km of single-mode fiber and receive by a two-channel
non-sliced optical arbitrary waveform measurement (OAWM) system, achieving
excellent signal quality. We believe that our scheme can unlock the full
potential of OAWG and disrupt a wide range of applications in high-speed
optical communications, photonic-electronic digital-to-analog conversion, as
well as advanced test and measurement in science and industry.",2024-12-12 18:54:32+00:00,"['Daniel Drayss', 'Dengyang Fang', 'Alban Sherifaj', 'Huanfa Peng', 'Christoph Füllner', 'Thomas Henauer', 'Grigory Lihachev', 'Tobias Harter', 'Wolfgang Freude', 'Sebastian Randel', 'Tobias J. Kippenberg', 'Thomas Zwick', 'Christian Koos']",conventional way generate optical waveform rely phase quadrature iq modulation continuous wave cw laser tone case bandwidth result optical waveform limit underlie electronic component particular digital analog converter dacs generate drive signal iq modulator bandwidth bottleneck overcome concept know optical arbitrary waveform generation oawg multiple iq modulator dac operate parallel synthesize individual spectral slice subsequently combine form single ultra broadband arbitrary optical waveform target synthesis arbitrary optical waveform multiple spectral slice far hamper difficulty maintain correct optical phase relationship slice paper propose demonstrate spectrally slice oawg active phase stabilization permit target synthesis truly arbitrary optical waveform demonstrate viability scheme synthesize optical waveform record high bandwidth ghz individually generate optical tributary proof concept experiment use oawg system generate datum signal symbol rate gbd transmit km single mode fiber receive channel non sliced optical arbitrary waveform measurement oawm system achieve excellent signal quality believe scheme unlock potential oawg disrupt wide range application high speed optical communication photonic electronic digital analog conversion advanced test measurement science industry
A Theoretical Analysis of Soft-Label vs Hard-Label Training in Neural Networks,"Knowledge distillation, where a small student model learns from a pre-trained
large teacher model, has achieved substantial empirical success since the
seminal work of \citep{hinton2015distilling}. Despite prior theoretical studies
exploring the benefits of knowledge distillation, an important question remains
unanswered: why does soft-label training from the teacher require significantly
fewer neurons than directly training a small neural network with hard labels?
To address this, we first present motivating experimental results using simple
neural network models on a binary classification problem. These results
demonstrate that soft-label training consistently outperforms hard-label
training in accuracy, with the performance gap becoming more pronounced as the
dataset becomes increasingly difficult to classify. We then substantiate these
observations with a theoretical contribution based on two-layer neural network
models. Specifically, we show that soft-label training using gradient descent
requires only $O\left(\frac{1}{\gamma^2 \epsilon}\right)$ neurons to achieve a
classification loss averaged over epochs smaller than some $\epsilon > 0$,
where $\gamma$ is the separation margin of the limiting kernel. In contrast,
hard-label training requires $O\left(\frac{1}{\gamma^4} \cdot
\ln\left(\frac{1}{\epsilon}\right)\right)$ neurons, as derived from an adapted
version of the gradient descent analysis in \citep{ji2020polylogarithmic}. This
implies that when $\gamma \leq \epsilon$, i.e., when the dataset is challenging
to classify, the neuron requirement for soft-label training can be
significantly lower than that for hard-label training. Finally, we present
experimental results on deep neural networks, further validating these
theoretical findings.",2024-12-12 18:54:07+00:00,"['Saptarshi Mandal', 'Xiaojun Lin', 'R. Srikant']",knowledge distillation small student model learn pre train large teacher model achieve substantial empirical success seminal work despite prior theoretical study explore benefit knowledge distillation important question remain unanswered soft label training teacher require significantly few neuron directly train small neural network hard label address present motivate experimental result simple neural network model binary classification problem result demonstrate soft label training consistently outperform hard label training accuracy performance gap pronounced dataset increasingly difficult classify substantiate observation theoretical contribution base layer neural network model specifically soft label training gradient descent require neuron achieve classification loss average epoch small separation margin limit kernel contrast hard label training require neuron derive adapt version gradient descent analysis imply dataset challenge classify neuron requirement soft label training significantly low hard label training finally present experimental result deep neural network validate theoretical finding
DISHONEST: Dissecting misInformation Spread using Homogeneous sOcial NEtworks and Semantic Topic classification,"The emergence of the COVID-19 pandemic resulted in a significant rise in the
spread of misinformation on online platforms such as Twitter. Oftentimes this
growth is blamed on the idea of the ""echo chamber."" However, the behavior said
to characterize these echo chambers exists in two dimensions. The first is in a
user's social interactions, where they are said to stick with the same clique
of like-minded users. The second is in the content of their posts, where they
are said to repeatedly espouse homogeneous ideas. In this study, we link the
two by using Twitter's network of retweets to study social interactions and
topic modeling to study tweet content. In order to measure the diversity of a
user's interactions over time, we develop a novel metric to track the speed at
which they travel through the social network. The application of these analysis
methods to misinformation-focused data from the pandemic demonstrates
correlation between social behavior and tweet content. We believe this
correlation supports the common intuition about how antisocial users behave,
and further suggests that it holds even in subcommunities already rife with
misinformation.",2024-12-12 18:53:46+00:00,"['Caleb Stam', 'Emily Saldanha', 'Mahantesh Halappanavar', 'Anurag Acharya']",emergence pandemic result significant rise spread misinformation online platform twitter oftentimes growth blame idea echo chamber behavior say characterize echo chamber exist dimension user social interaction say stick clique like minded user second content post say repeatedly espouse homogeneous idea study link twitter network retweet study social interaction topic modeling study tweet content order measure diversity user interaction time develop novel metric track speed travel social network application analysis method misinformation focus datum pandemic demonstrate correlation social behavior tweet content believe correlation support common intuition antisocial user behave suggest hold subcommunitie rife misinformation
JuStRank: Benchmarking LLM Judges for System Ranking,"Given the rapid progress of generative AI, there is a pressing need to
systematically compare and choose between the numerous models and
configurations available. The scale and versatility of such evaluations make
the use of LLM-based judges a compelling solution for this challenge.
Crucially, this approach requires first to validate the quality of the LLM
judge itself. Previous work has focused on instance-based assessment of LLM
judges, where a judge is evaluated over a set of responses, or response pairs,
while being agnostic to their source systems. We argue that this setting
overlooks critical factors affecting system-level ranking, such as a judge's
positive or negative bias towards certain systems. To address this gap, we
conduct the first large-scale study of LLM judges as system rankers. System
scores are generated by aggregating judgment scores over multiple system
outputs, and the judge's quality is assessed by comparing the resulting system
ranking to a human-based ranking. Beyond overall judge assessment, our analysis
provides a fine-grained characterization of judge behavior, including their
decisiveness and bias.",2024-12-12 18:51:13+00:00,"['Ariel Gera', 'Odellia Boni', 'Yotam Perlitz', 'Roy Bar-Haim', 'Lilach Eden', 'Asaf Yehudai']",give rapid progress generative ai press need systematically compare choose numerous model configuration available scale versatility evaluation use llm base judge compelling solution challenge crucially approach require validate quality llm judge previous work focus instance base assessment llm judge judge evaluate set response response pair agnostic source system argue set overlook critical factor affect system level ranking judge positive negative bias certain system address gap conduct large scale study llm judge system ranker system score generate aggregate judgment score multiple system output judge quality assess compare result system rank human base ranking overall judge assessment analysis provide fine grain characterization judge behavior include decisiveness bias
A glitch in gravity: cosmic Lorentz-violation from fiery Big Bang to glacial heat death,"One regime where we might see departures from general relativity is at the
largest accessible scales, with a natural choice in cosmology being the
cosmological horizon (or Hubble) scale. We investigate a single-parameter
extension to the standard cosmological model with a different strength of
gravity above and below this scale -- a ""cosmic glitch"" in gravity. Cosmic
microwave background observations, and Baryonic Acoustic Oscillations
(including the recent DESI Y1) favour weaker superhorizon gravity, at nearly a
percent (or 2$\sigma$ level), easing both the Hubble and clustering tensions
with other cosmological data. This compounds evidence for an even stronger
glitch during Big Bang nucleosynthesis (from helium abundance observations),
suggesting that symmetries of general relativity are maximally violated at the
Big Bang, but gradually recovered as we approach the present-day cosmological
de Sitter scale, associated with the observed dark energy.",2024-12-12 18:50:56+00:00,"['Robin Y. Wen', 'Lukas T. Hergt', 'Niayesh Afshordi', 'Douglas Scott']",regime departure general relativity large accessible scale natural choice cosmology cosmological horizon hubble scale investigate single parameter extension standard cosmological model different strength gravity scale cosmic glitch gravity cosmic microwave background observation baryonic acoustic oscillations include recent desi favour weak superhorizon gravity nearly percent level ease hubble cluster tension cosmological datum compound evidence strong glitch big bang nucleosynthesis helium abundance observation suggest symmetry general relativity maximally violate big bang gradually recover approach present day cosmological de sitter scale associate observe dark energy
Temporal Triadic Closure: Finding Dense Structures in Social Networks That Evolve,"A graph G is c-closed if every two vertices with at least c common neighbors
are adjacent to each other. Introduced by Fox, Roughgarden, Seshadhri, Wei and
Wein [ICALP 2018, SICOMP 2020], this definition is an abstraction of the
triadic closure property exhibited by many real-world social networks, namely,
friends of friends tend to be friends themselves. Social networks, however, are
often temporal rather than static -- the connections change over a period of
time. And hence temporal graphs, rather than static graphs, are often better
suited to model social networks. Motivated by this, we introduce a definition
of temporal c-closed graphs, in which if two vertices u and v have at least c
common neighbors during a short interval of time, then u and v are adjacent to
each other around that time. Our pilot experiments show that several real-world
temporal networks are c-closed for rather small values of c. We also study the
computational problems of enumerating maximal cliques and similar dense
subgraphs in temporal c-closed graphs; a clique in a temporal graph is a
subgraph that lasts for a certain period of time, during which every possible
edge in the subgraph becomes active often enough, and other dense subgraphs are
defined similarly. We bound the number of such maximal dense subgraphs in a
temporal c-closed graph that evolves slowly, and thus show that the
corresponding enumeration problems admit efficient algorithms; by slow
evolution, we mean that between consecutive time-steps, the local change in
adjacencies remains small. Our work also adds to a growing body of literature
on defining suitable structural parameters for temporal graphs that can be
leveraged to design efficient algorithms.",2024-12-12 18:50:55+00:00,"['Tom Davot', 'Jessica Enright', 'Jayakrishnan Madathil', 'Kitty Meeks']",graph g c closed vertex c common neighbor adjacent introduce fox roughgarden seshadhri wei wein icalp sicomp definition abstraction triadic closure property exhibit real world social network friend friend tend friend social network temporal static connection change period time temporal graph static graph well suit model social network motivate introduce definition temporal c closed graph vertice u v c common neighbor short interval time u v adjacent time pilot experiment real world temporal network c close small value study computational problem enumerate maximal clique similar dense subgraph temporal c close graph clique temporal graph subgraph last certain period time possible edge subgraph active dense subgraph define similarly bind number maximal dense subgraph temporal c close graph evolve slowly corresponding enumeration problem admit efficient algorithm slow evolution mean consecutive time step local change adjacency remain small work add grow body literature define suitable structural parameter temporal graph leverage design efficient algorithm
Obfuscated Activations Bypass LLM Latent-Space Defenses,"Recent latent-space monitoring techniques have shown promise as defenses
against LLM attacks. These defenses act as scanners that seek to detect harmful
activations before they lead to undesirable actions. This prompts the question:
Can models execute harmful behavior via inconspicuous latent states? Here, we
study such obfuscated activations. We show that state-of-the-art latent-space
defenses -- including sparse autoencoders, representation probing, and latent
OOD detection -- are all vulnerable to obfuscated activations. For example,
against probes trained to classify harmfulness, our attacks can often reduce
recall from 100% to 0% while retaining a 90% jailbreaking rate. However,
obfuscation has limits: we find that on a complex task (writing SQL code),
obfuscation reduces model performance. Together, our results demonstrate that
neural activations are highly malleable: we can reshape activation patterns in
a variety of ways, often while preserving a network's behavior. This poses a
fundamental challenge to latent-space defenses.",2024-12-12 18:49:53+00:00,"['Luke Bailey', 'Alex Serrano', 'Abhay Sheshadri', 'Mikhail Seleznyov', 'Jordan Taylor', 'Erik Jenner', 'Jacob Hilton', 'Stephen Casper', 'Carlos Guestrin', 'Scott Emmons']",recent latent space monitoring technique show promise defense llm attack defense act scanner seek detect harmful activation lead undesirable action prompt question model execute harmful behavior inconspicuous latent state study obfuscated activation state art latent space defense include sparse autoencoder representation probe latent ood detection vulnerable obfuscate activation example probe train classify harmfulness attack reduce recall retain jailbreaking rate obfuscation limit find complex task write sql code obfuscation reduce model performance result demonstrate neural activation highly malleable reshape activation pattern variety way preserve network behavior pose fundamental challenge latent space defense
Improving the Reliability of Cable Broadband Networks via Proactive Network Maintenance,"Cable broadband networks are one of the few ""last-mile"" broadband
technologies widely available in the U.S. Unfortunately, they have poor
reliability after decades of deployment. The cable industry proposed a
framework called Proactive Network Maintenance (PNM) to diagnose the cable
networks. However, there is little public knowledge or systematic study on how
to use these data to detect and localize cable network problems. Existing tools
in the public domain have prohibitive high false-positive rates. In this paper,
we propose CableMon, the first public-domain system that applies machine
learning techniques to PNM data to improve the reliability of cable broadband
networks. CableMon tackles two key challenges faced by cable ISPs: accurately
detecting failures, and distinguishing whether a failure occurs within a
network or at a subscriber's premise. CableMon uses statistical models to
generate features from time series data and uses customer trouble tickets as
hints to infer abnormal/failure thresholds for these generated features.
Further, CableMon employs an unsupervised learning model to group cable devices
sharing similar anomalous patterns and effectively identify impairments that
occur inside a cable network and impairments occur at a subscriber's premise,
as these two different faults require different types of technical personnel to
repair them. We use eight months of PNM data and customer trouble tickets from
an ISP and experimental deployment to evaluate CableMon's performance. Our
evaluation results show that CableMon can effectively detect and distinguish
failures from PNM data and outperforms existing public-domain tools.",2024-12-12 18:49:11+00:00,"['Jiyao Hu', 'Zhenyu Zhou', 'Xiaowei Yang']",cable broadband network mile broadband technology widely available unfortunately poor reliability decade deployment cable industry propose framework call proactive network maintenance pnm diagnose cable network little public knowledge systematic study use datum detect localize cable network problem exist tool public domain prohibitive high false positive rate paper propose cablemon public domain system apply machine learn technique pnm datum improve reliability cable broadband network cablemon tackle key challenge face cable isp accurately detecting failure distinguish failure occur network subscriber premise cablemon use statistical model generate feature time series datum use customer trouble ticket hint infer abnormal failure threshold generate feature cablemon employ unsupervised learn model group cable device share similar anomalous pattern effectively identify impairment occur inside cable network impairment occur subscriber premise different fault require different type technical personnel repair use month pnm datum customer trouble ticket isp experimental deployment evaluate cablemon performance evaluation result cablemon effectively detect distinguish failure pnm datum outperform exist public domain tool
Does Representation Matter? Exploring Intermediate Layers in Large Language Models,"Understanding what defines a good representation in large language models
(LLMs) is fundamental to both theoretical understanding and practical
applications. In this paper, we investigate the quality of intermediate
representations in various LLM architectures, including Transformers and State
Space Models (SSMs). We find that intermediate layers often yield more
informative representations for downstream tasks than the final layers. To
measure the representation quality, we adapt and apply a suite of metrics -
such as prompt entropy, curvature, and augmentation-invariance - originally
proposed in other contexts. Our empirical study reveals significant
architectural differences, how representations evolve throughout training, and
how factors like input randomness and prompt length affect each layer. Notably,
we observe a bimodal pattern in the entropy of some intermediate layers and
consider potential explanations tied to training data. Overall, our results
illuminate the internal mechanics of LLMs and guide strategies for
architectural optimization and training.",2024-12-12 18:48:51+00:00,"['Oscar Skean', 'Md Rifat Arefin', 'Yann LeCun', 'Ravid Shwartz-Ziv']",understand define good representation large language model llms fundamental theoretical understanding practical application paper investigate quality intermediate representation llm architecture include transformers state space models ssms find intermediate layer yield informative representation downstream task final layer measure representation quality adapt apply suite metric prompt entropy curvature augmentation invariance originally propose contexts empirical study reveal significant architectural difference representation evolve training factor like input randomness prompt length affect layer notably observe bimodal pattern entropy intermediate layer consider potential explanation tie training datum overall result illuminate internal mechanic llms guide strategy architectural optimization training
Identification of structures driving trailing-edge noise. Part II -- Numerical investigation,"The aim of the present work is to investigate the mechanisms of broadband
trailing-edge noise generation to improve prediction tools and control
strategies. We focus on a NACA 0012 airfoil at 3 degrees angle of attack and
chord Reynolds number Re = 200,000. A high-fidelity wall-resolved compressible
implicit large eddy simulation (LES) is performed to collect data for our
analysis. The simulation is designed in close alignment with the experiment
described in detail in the companion paper (Demange et al. 2024b). Zig-zag
geometrical tripping elements, added to generate a turbulent boundary layer,
are meshed to closely follow the experimental setup. A large spanwise domain is
used in the simulation to include propagative acoustic waves with low
wavenumbers. An in-depth comparison with experiments is conducted showing good
agreement in terms of mean flow statistics, acoustic and hydrodynamic spectra,
and coherence lengths. Furthermore, a strong correlation is found between the
radiated acoustics and spanwise-coherent structures. To investigate the
correlation for higher wavenumbers, spectral proper orthogonal decomposition
(SPOD) is applied to the spanwise Fourier-transformed LES dataset. The analysis
of all SPOD modes for the leading spanwise wavenumbers reveals
streamwise-travelling wavepackets as the source of the radiated acoustics. This
finding, confirming observations from experiments in the companion paper, leads
to a new understanding of the turbulent structures driving the trailing-edge
noise. By performing extended SPOD based on the acoustic region, we confirm the
low rank nature of the acoustics, and a reduced-order model based on acoustic
extended SPOD is proposed for the far-field acoustic reconstruction.",2024-12-12 18:47:43+00:00,"['Zhenyang Yuan', 'Simon Demange', 'Kilian Oberleithner', 'André V. G. Cavalieri', 'Ardeshir Hanifi']",aim present work investigate mechanism broadband trail edge noise generation improve prediction tool control strategy focus naca airfoil degree angle attack chord reynolds number high fidelity wall resolve compressible implicit large eddy simulation les perform collect datum analysis simulation design close alignment experiment describe detail companion paper demange et al zig zag geometrical trip element add generate turbulent boundary layer mesh closely follow experimental setup large spanwise domain simulation include propagative acoustic wave low wavenumber depth comparison experiment conduct show good agreement term mean flow statistic acoustic hydrodynamic spectra coherence length furthermore strong correlation find radiate acoustic spanwise coherent structure investigate correlation high wavenumber spectral proper orthogonal decomposition spod apply spanwise fourier transform les dataset analysis spod mode lead spanwise wavenumber reveal streamwise travel wavepacket source radiate acoustic finding confirm observation experiment companion paper lead new understanding turbulent structure drive trail edge noise perform extend spod base acoustic region confirm low rank nature acoustic reduce order model base acoustic extended spod propose far field acoustic reconstruction
Two-dimensional orbital-obstructed insulators with higher-order band topology,"Obstructed atomic phases, with their realizations in systems of diverse
dimensionality, have recently arisen as one of the topological states with
greatest potential to show higher-order phenomena. In this work we report a
special type of obstruction, known as orbital-mediated atomic obstruction, in
monolayers of materials with spatial symmetry described by the space group
P-3m1. By means of a minimal tight-binding model and first-principles
calculations, we show that this obstructed phase is related to the mismatch of
the charge centers coming from the atomic limit with respect to the centers
that are obtained from the reciprocal space description. Although we find
atomic limits that correspond with occupied atomic sites, orbital-mediated
atomic obstruction requires the presence of orbitals that have no support in
real space. In order to demonstrate the nontrivial character of the
obstruction, we confirm the presence of a filling anomaly for finite geometries
that is directly associated with the bulk configuration, and discuss the role
of the boundary states and their underlying mechanism. Several material
examples are presented to illustrate the ubiquity of these nontrivial responses
and, in turn, to discuss the differences related to the particular ground state
configuration. In addition, we perform a survey of materials and elaborate a
list of candidate systems which will host this obstructed phase in monolayer
form.",2024-12-12 18:46:48+00:00,"['Olga Arroyo-Gascon', 'Sergio Bravo', 'Monica Pacheco', 'Leonor Chico']",obstruct atomic phase realization system diverse dimensionality recently arise topological state great potential high order phenomenon work report special type obstruction know orbital mediate atomic obstruction monolayer material spatial symmetry describe space group mean minimal tight bind model principle calculation obstruct phase relate mismatch charge center come atomic limit respect center obtain reciprocal space description find atomic limit correspond occupy atomic site orbital mediate atomic obstruction require presence orbital support real space order demonstrate nontrivial character obstruction confirm presence fill anomaly finite geometry directly associate bulk configuration discuss role boundary state underlying mechanism material example present illustrate ubiquity nontrivial response turn discuss difference relate particular ground state configuration addition perform survey material elaborate list candidate system host obstruct phase monolayer form
Foundational Large Language Models for Materials Research,"Materials discovery and development are critical for addressing global
challenges. Yet, the exponential growth in materials science literature
comprising vast amounts of textual data has created significant bottlenecks in
knowledge extraction, synthesis, and scientific reasoning. Large Language
Models (LLMs) offer unprecedented opportunities to accelerate materials
research through automated analysis and prediction. Still, their effective
deployment requires domain-specific adaptation for understanding and solving
domain-relevant tasks. Here, we present LLaMat, a family of foundational models
for materials science developed through continued pretraining of LLaMA models
on an extensive corpus of materials literature and crystallographic data.
Through systematic evaluation, we demonstrate that LLaMat excels in
materials-specific NLP and structured information extraction while maintaining
general linguistic capabilities. The specialized LLaMat-CIF variant
demonstrates unprecedented capabilities in crystal structure generation,
predicting stable crystals with high coverage across the periodic table.
Intriguingly, despite LLaMA-3's superior performance in comparison to LLaMA-2,
we observe that LLaMat-2 demonstrates unexpectedly enhanced domain-specific
performance across diverse materials science tasks, including structured
information extraction from text and tables, more particularly in crystal
structure generation, a potential adaptation rigidity in overtrained LLMs.
Altogether, the present work demonstrates the effectiveness of domain
adaptation towards developing practically deployable LLM copilots for materials
research. Beyond materials science, our findings reveal important
considerations for domain adaptation of LLMs, such as model selection, training
methodology, and domain-specific performance, which may influence the
development of specialized scientific AI systems.",2024-12-12 18:46:38+00:00,"['Vaibhav Mishra', 'Somaditya Singh', 'Dhruv Ahlawat', 'Mohd Zaki', 'Vaibhav Bihani', 'Hargun Singh Grover', 'Biswajit Mishra', 'Santiago Miret', 'Mausam', 'N. M. Anoop Krishnan']",material discovery development critical address global challenge exponential growth material science literature comprise vast amount textual datum create significant bottleneck knowledge extraction synthesis scientific reasoning large language models llms offer unprecedented opportunity accelerate material research automate analysis prediction effective deployment require domain specific adaptation understanding solve domain relevant task present llamat family foundational model material science develop continue pretraining llama model extensive corpus material literature crystallographic datum systematic evaluation demonstrate llamat excel material specific nlp structured information extraction maintain general linguistic capability specialized llamat cif variant demonstrate unprecedented capability crystal structure generation predict stable crystal high coverage periodic table intriguingly despite superior performance comparison observe demonstrate unexpectedly enhance domain specific performance diverse material science task include structure information extraction text table particularly crystal structure generation potential adaptation rigidity overtrained llms altogether present work demonstrate effectiveness domain adaptation develop practically deployable llm copilot material research material science finding reveal important consideration domain adaptation llms model selection training methodology domain specific performance influence development specialized scientific ai system
Experimental Machine Learning with Classical and Quantum Data via NMR Quantum Kernels,"Kernel methods map data into high-dimensional spaces, enabling linear
algorithms to learn nonlinear functions without explicitly storing the feature
vectors. Quantum kernel methods promise efficient learning by encoding feature
maps into exponentially large Hilbert spaces inherent in quantum systems. In
this work we implement quantum kernels on a 10-qubit star-topology register in
a nuclear magnetic resonance (NMR) platform. We experimentally encode classical
data in the evolution of multiple quantum coherence orders using data-dependent
unitary transformations and then demonstrate one-dimensional regression and
two-dimensional classification tasks. By extending the register to a
double-layered star configuration, we propose an extended quantum kernel to
handle non-parametrized operator inputs. By numerically simulating the extended
quantum kernel, we show classification of entangling and nonentangling
unitaries. These results confirm that quantum kernels exhibit strong
capabilities in classical as well as quantum machine learning tasks.",2024-12-12 18:44:38+00:00,"['Vivek Sabarad', 'T. S. Mahesh']",kernel method map datum high dimensional space enable linear algorithm learn nonlinear function explicitly store feature vector quantum kernel method promise efficient learning encode feature map exponentially large hilbert space inherent quantum system work implement quantum kernel qubit star topology register nuclear magnetic resonance nmr platform experimentally encode classical datum evolution multiple quantum coherence order data dependent unitary transformation demonstrate dimensional regression dimensional classification task extend register double layered star configuration propose extend quantum kernel handle non parametrized operator input numerically simulate extend quantum kernel classification entangle nonentangling unitarie result confirm quantum kernels exhibit strong capability classical quantum machine learn task
Enhancing Convergence of Decentralized Gradient Tracking under the KL Property,"We study decentralized multiagent optimization over networks, modeled as
undirected graphs. The optimization problem consists of minimizing a nonconvex
smooth function plus a convex extended-value function, which enforces
constraints or extra structure on the solution (e.g., sparsity, low-rank). We
further assume that the objective function satisfies the Kurdyka-{\L}ojasiewicz
(KL) property, with given exponent $\theta\in [0,1)$. The KL property is
satisfied by several (nonconvex) functions of practical interest, e.g., arising
from machine learning applications; in the centralized setting, it permits to
achieve strong convergence guarantees. Here we establish convergence of the
same type for the notorious decentralized gradient-tracking-based algorithm
SONATA. Specifically, $\textbf{(i)}$ when $\theta\in (0,1/2]$, the sequence
generated by SONATA converges to a stationary solution of the problem at
R-linear rate;$ \textbf{(ii)} $when $\theta\in (1/2,1)$, sublinear rate is
certified; and finally $\textbf{(iii)}$ when $\theta=0$, the iterates will
either converge in a finite number of steps or converges at R-linear rate. This
matches the convergence behavior of centralized proximal-gradient algorithms
except when $\theta=0$. Numerical results validate our theoretical findings.",2024-12-12 18:44:36+00:00,"['Xiaokai Chen', 'Tianyu Cao', 'Gesualdo Scutari']",study decentralize multiagent optimization network model undirected graph optimization problem consist minimize nonconvex smooth function plus convex extend value function enforce constraint extra structure solution sparsity low rank assume objective function satisfy kl property give exponent kl property satisfied nonconvex function practical interest arise machine learning application centralized setting permit achieve strong convergence guarantee establish convergence type notorious decentralize gradient tracking base algorithm sonata specifically sequence generate sonata converge stationary solution problem r linear sublinear rate certify finally iterate converge finite number step converge r linear rate match convergence behavior centralized proximal gradient algorithm numerical result validate theoretical finding
Network Dynamics of Emotional Processing: A Structural Balance Theory Approach,"Understanding emotional processing in the human brain requires examining the
complex interactions between different brain regions. While previous studies
have identified specific regions involved in emotion processing, a holistic
network approach may provide deeper insights. We use Structural Balance Theory
to investigate the stability and triadic structures of signed brain networks
during resting state and emotional processing, specifically in response to
fear-related stimuli. We hypothesized that imbalanced triadic interactions
would be more prevalent during emotional processing, especially in response to
fear-related stimuli, potentially reflecting the brain's adaptation to
emotional challenges. By analyzing fMRI data from 138 healthy, right-handed
participants, we found that emotional processing was marked by an increase in
positive connections and a decrease in negative connections compared to the
resting state. Our findings clearly show that balanced triads significantly
decreased while imbalanced triads increased, indicating a shift toward
instability in the brain's functional network during emotional processing.
Additionally, the number of influential hubs was significantly lower during
fear processing than in neutral conditions, suggesting a more centralized
network and higher levels of network energy. These findings reveal the brain's
remarkable adaptive capacity during emotional processing, demonstrating how
network stability dynamically shifts through changes in balanced and imbalanced
triads, hub tendencies, and energy dynamics. Our research illuminates a complex
mechanism by which the brain flexibly reconfigures its functional network in
response to emotional stimuli with potential implications for understanding
emotional resilience and neurological disorders.",2024-12-12 18:43:41+00:00,"['Sepehr Gourabi', 'Parinaz Khosravani', 'Shahrzad Nosrat', 'Roya Mohammadi', 'Masoud Lotfalipour']",understand emotional processing human brain require examine complex interaction different brain region previous study identify specific region involve emotion processing holistic network approach provide deep insight use structural balance theory investigate stability triadic structure sign brain network rest state emotional processing specifically response fear relate stimulus hypothesize imbalance triadic interaction prevalent emotional processing especially response fear relate stimulus potentially reflect brain adaptation emotional challenge analyze fmri datum healthy right handed participant find emotional processing mark increase positive connection decrease negative connection compare resting state finding clearly balanced triad significantly decrease imbalance triad increase indicate shift instability brain functional network emotional processing additionally number influential hub significantly low fear processing neutral condition suggest centralized network high level network energy finding reveal brain remarkable adaptive capacity emotional processing demonstrate network stability dynamically shift change balanced imbalanced triad hub tendency energy dynamic research illuminate complex mechanism brain flexibly reconfigure functional network response emotional stimulus potential implication understand emotional resilience neurological disorder
Periodic splay Fréedericksz transitions in a ferroelectric nematic,"Electric field-induced splay of molecular orientation, called the
Fr\'eedericksz transition, is a fundamental electro-optic phenomenon in
nonpolar nematic liquid crystals. In a ferroelectric nematic NF with a
spontaneous electric polarization P, the splay is suppressed since it produces
bound electric charges. Here, we demonstrate that an alternating current (ac)
electric field causes three patterns of NF polarization. At low voltages, P
oscillates around the field-free orientation with no stationary deformations.
As the voltage increases, the polarization acquires stationary distortions,
first splay and twist in a stripe pattern and then splay and bend in a square
lattice of +1 and -1 defects. In all patterns, P oscillates around the
stationary orientations. The stationary bound charge is reduced by a
geometrical splay cancellation mechanism that does not require free ions: the
charge created by splay in one plane is reduced by splay of an opposite sign in
the orthogonal plane.",2024-12-12 18:43:17+00:00,"['Bijaya Basnet', 'Sathyanarayana Paladugu', 'Oleksandr Kurochkin', 'Oleksandr Buluy', 'Natalie Aryasova', 'Vassili G. Nazarenko', 'Sergij V. Shiyanovskii', 'Oleg D. Lavrentovich']",electric field induce splay molecular orientation call transition fundamental electro optic phenomenon nonpolar nematic liquid crystal ferroelectric nematic nf spontaneous electric polarization p splay suppress produce bind electric charge demonstrate alternate current ac electric field cause pattern nf polarization low voltage p oscillate field free orientation stationary deformation voltage increase polarization acquire stationary distortion splay twist stripe pattern splay bend square lattice defect pattern p oscillate stationary orientation stationary bind charge reduce geometrical splay cancellation mechanism require free ion charge create splay plane reduce splay opposite sign orthogonal plane
Video Creation by Demonstration,"We explore a novel video creation experience, namely Video Creation by
Demonstration. Given a demonstration video and a context image from a different
scene, we generate a physically plausible video that continues naturally from
the context image and carries out the action concepts from the demonstration.
To enable this capability, we present $\delta$-Diffusion, a self-supervised
training approach that learns from unlabeled videos by conditional future frame
prediction. Unlike most existing video generation controls that are based on
explicit signals, we adopts the form of implicit latent control for maximal
flexibility and expressiveness required by general videos. By leveraging a
video foundation model with an appearance bottleneck design on top, we extract
action latents from demonstration videos for conditioning the generation
process with minimal appearance leakage. Empirically, $\delta$-Diffusion
outperforms related baselines in terms of both human preference and large-scale
machine evaluations, and demonstrates potentials towards interactive world
simulation. Sampled video generation results are available at
https://delta-diffusion.github.io/.",2024-12-12 18:41:20+00:00,"['Yihong Sun', 'Hao Zhou', 'Liangzhe Yuan', 'Jennifer J. Sun', 'Yandong Li', 'Xuhui Jia', 'Hartwig Adam', 'Bharath Hariharan', 'Long Zhao', 'Ting Liu']",explore novel video creation experience video creation demonstration give demonstration video context image different scene generate physically plausible video continue naturally context image carry action concept demonstration enable capability present self supervise training approach learn unlabeled video conditional future frame prediction unlike exist video generation control base explicit signal adopt form implicit latent control maximal flexibility expressiveness require general video leverage video foundation model appearance bottleneck design extract action latent demonstration video condition generation process minimal appearance leakage empirically outperform related baseline term human preference large scale machine evaluation demonstrate potential interactive world simulation sample video generation result available
Ferromagnetic ordering in mazelike stripe liquid of a dipolar six-state clock model,"We present a comprehensive numerical study of a six-state clock model with a
long-range dipolar type interaction. This model is motivated by the
ferroelectric orders in the multiferroic hexagonal manganites. At low
temperatures, trimerization of local atomic structures leads to six distinct
but energetically degenerate structural distortion, which can be modeled by a
six-state clock model. Moreover, the atomic displacements in the trimerized
state further produce a local electric polarization whose sign depends on
whether the clock variable is even or odd. These induced electric dipoles,
which can be modeled by emergent Ising degrees of freedom, interact with each
other via long-range dipolar interactions. Extensive Monte Carlo simulations
are carried out to investigate low temperature phases resulting from the
competing interactions. Upon lowering temperature, the system undergoes two
Berezinskii-Kosterlitz-Thouless (BKT) transitions, characteristic of the
standard six-state clock model in two dimensions. The dipolar interaction
between emergent Ising spins induces a first-order transition into a ground
state characterized by a three-fold degenerate stripe order. The intermediate
phase between the discontinuous and the second BKT transition corresponds to a
maze-like hexagonal liquid with short-range stripe ordering. Moreover, this
intermediate phase also exhibits an unusual ferromagnetic order with two
adjacent clock variables occupying the two types of stripes of the labyrinthine
pattern.",2024-12-12 18:40:30+00:00,"['Quentin Simmons', 'Shi-Zeng Lin', 'Gia-Wei Chern']",present comprehensive numerical study state clock model long range dipolar type interaction model motivate ferroelectric order multiferroic hexagonal manganite low temperature trimerization local atomic structure lead distinct energetically degenerate structural distortion model state clock model atomic displacement trimerized state produce local electric polarization sign depend clock variable odd induce electric dipole model emergent ising degree freedom interact long range dipolar interaction extensive monte carlo simulation carry investigate low temperature phase result compete interaction lower temperature system undergo berezinskii kosterlitz thouless bkt transition characteristic standard state clock model dimension dipolar interaction emergent ising spin induce order transition ground state characterize fold degenerate stripe order intermediate phase discontinuous second bkt transition correspond maze like hexagonal liquid short range stripe ordering intermediate phase exhibit unusual ferromagnetic order adjacent clock variable occupy type stripe labyrinthine pattern
Exemplar Masking for Multimodal Incremental Learning,"Multimodal incremental learning needs to digest the information from multiple
modalities while concurrently learning new knowledge without forgetting the
previously learned information. There are numerous challenges for this task,
mainly including the larger storage size of multimodal data in exemplar-based
methods and the computational requirement of finetuning on huge multimodal
models. In this paper, we leverage the parameter-efficient tuning scheme to
reduce the burden of fine-tuning and propose the exemplar masking framework to
efficiently replay old knowledge. Specifically, the non-important tokens are
masked based on the attention weights and the correlation across different
modalities, significantly reducing the storage size of an exemplar and
consequently saving more exemplars under the same memory buffer. Moreover, we
design a multimodal data augmentation technique to diversify exemplars for
replaying prior knowledge. In experiments, we not only evaluate our method in
existing multimodal datasets but also extend the ImageNet-R dataset to a
multimodal dataset as a real-world application, where captions are generated by
querying multimodal large language models (e.g., InstructBLIP). Extensive
experiments show that our exemplar masking framework is more efficient and
robust to catastrophic forgetting under the same limited memory buffer. Code is
available at https://github.com/YiLunLee/Exemplar_Masking_MCIL.",2024-12-12 18:40:20+00:00,"['Yi-Lun Lee', 'Chen-Yu Lee', 'Wei-Chen Chiu', 'Yi-Hsuan Tsai']",multimodal incremental learning need digest information multiple modality concurrently learn new knowledge forget previously learn information numerous challenge task mainly include large storage size multimodal datum exemplar base method computational requirement finetune huge multimodal model paper leverage parameter efficient tuning scheme reduce burden fine tuning propose exemplar mask framework efficiently replay old knowledge specifically non important token mask base attention weight correlation different modality significantly reduce storage size exemplar consequently save exemplar memory buffer design multimodal datum augmentation technique diversify exemplar replay prior knowledge experiment evaluate method exist multimodal dataset extend imagenet r dataset multimodal dataset real world application caption generate query multimodal large language model instructblip extensive experiment exemplar masking framework efficient robust catastrophic forgetting limited memory buffer code available
SimAvatar: Simulation-Ready Avatars with Layered Hair and Clothing,"We introduce SimAvatar, a framework designed to generate simulation-ready
clothed 3D human avatars from a text prompt. Current text-driven human avatar
generation methods either model hair, clothing, and the human body using a
unified geometry or produce hair and garments that are not easily adaptable for
simulation within existing simulation pipelines. The primary challenge lies in
representing the hair and garment geometry in a way that allows leveraging
established prior knowledge from foundational image diffusion models (e.g.,
Stable Diffusion) while being simulation-ready using either physics or neural
simulators. To address this task, we propose a two-stage framework that
combines the flexibility of 3D Gaussians with simulation-ready hair strands and
garment meshes. Specifically, we first employ three text-conditioned 3D
generative models to generate garment mesh, body shape and hair strands from
the given text prompt. To leverage prior knowledge from foundational diffusion
models, we attach 3D Gaussians to the body mesh, garment mesh, as well as hair
strands and learn the avatar appearance through optimization. To drive the
avatar given a pose sequence, we first apply physics simulators onto the
garment meshes and hair strands. We then transfer the motion onto 3D Gaussians
through carefully designed mechanisms for each body part. As a result, our
synthesized avatars have vivid texture and realistic dynamic motion. To the
best of our knowledge, our method is the first to produce highly realistic,
fully simulation-ready 3D avatars, surpassing the capabilities of current
approaches.",2024-12-12 18:35:26+00:00,"['Xueting Li', 'Ye Yuan', 'Shalini De Mello', 'Gilles Daviet', 'Jonathan Leaf', 'Miles Macklin', 'Jan Kautz', 'Umar Iqbal']",introduce simavatar framework design generate simulation ready clothe human avatar text prompt current text drive human avatar generation method model hair clothing human body unified geometry produce hair garment easily adaptable simulation exist simulation pipeline primary challenge lie represent hair garment geometry way allow leverage establish prior knowledge foundational image diffusion model stable diffusion simulation ready physic neural simulator address task propose stage framework combine flexibility gaussians simulation ready hair strand garment mesh specifically employ text condition generative model generate garment mesh body shape hair strand give text prompt leverage prior knowledge foundational diffusion model attach gaussians body mesh garment mesh hair strand learn avatar appearance optimization drive avatar give pose sequence apply physics simulator garment mesh hair strand transfer motion gaussians carefully design mechanism body result synthesize avatar vivid texture realistic dynamic motion good knowledge method produce highly realistic fully simulation ready avatar surpass capability current approach
Sail into the Headwind: Alignment via Robust Rewards and Dynamic Labels against Reward Hacking,"Aligning AI systems with human preferences typically suffers from the
infamous reward hacking problem, where optimization of an imperfect reward
model leads to undesired behaviors. In this paper, we investigate reward
hacking in offline preference optimization, which aims to improve an initial
model using a preference dataset. We identify two types of reward hacking
stemming from statistical fluctuations in the dataset: Type I Reward Hacking
due to subpar choices appearing more favorable, and Type II Reward Hacking due
to decent choices appearing less favorable. We prove that many (mainstream or
theoretical) preference optimization methods suffer from both types of reward
hacking. To mitigate Type I Reward Hacking, we propose POWER, a new preference
optimization method that combines Guiasu's weighted entropy with a robust
reward maximization objective. POWER enjoys finite-sample guarantees under
general function approximation, competing with the best covered policy in the
data. To mitigate Type II Reward Hacking, we analyze the learning dynamics of
preference optimization and develop a novel technique that dynamically updates
preference labels toward certain ""stationary labels"", resulting in diminishing
gradients for untrustworthy samples. Empirically, POWER with dynamic labels
(POWER-DL) consistently outperforms state-of-the-art methods on alignment
benchmarks, achieving improvements of up to 13.0 points on AlpacaEval 2.0 and
11.5 points on Arena-Hard over DPO, while also improving or maintaining
performance on downstream tasks such as mathematical reasoning. Strong
theoretical guarantees and empirical results demonstrate the promise of
POWER-DL in mitigating reward hacking.",2024-12-12 18:34:47+00:00,"['Paria Rashidinejad', 'Yuandong Tian']",align ai system human preference typically suffer infamous reward hack problem optimization imperfect reward model lead undesired behavior paper investigate reward hack offline preference optimization aim improve initial model preference dataset identify type reward hack stem statistical fluctuation dataset type reward hacking subpar choice appear favorable type ii reward hack decent choice appear favorable prove mainstream theoretical preference optimization method suffer type reward hacking mitigate type reward hacking propose power new preference optimization method combine guiasu weighted entropy robust reward maximization objective power enjoy finite sample guarantee general function approximation compete well cover policy datum mitigate type ii reward hacking analyze learn dynamic preference optimization develop novel technique dynamically update preference label certain stationary label result diminish gradient untrustworthy sample empirically power dynamic label power dl consistently outperform state art method alignment benchmark achieve improvement point alpacaeval point arena hard dpo improve maintain performance downstream task mathematical reasoning strong theoretical guarantee empirical result demonstrate promise power dl mitigate reward hack
The Controlled Four-Parameter Method for Cross-Assignment of Directional Wave Systems,"Cross-assignment of directional wave spectra is a critical task in wave data
assimilation. Traditionally, most methods rely on two-parameter spectral
distances or energy ranking approaches, which often fail to account for the
complexities of the wave field, leading to inaccuracies. To address these
limitations, we propose the Controlled Four-Parameter Method (C4PM), which
independently considers four integrated wave parameters. This method enhances
the accuracy and robustness of cross-assignment by offering flexibility in
assigning weights and controls to each wave parameter. We compare C4PM with a
two-parameter spectral distance method using data from two buoys moored 13 km
apart in deep water. Although both methods produce negligible bias and high
correlation, C4PM demonstrates superior performance by preventing the
occurrence of outliers and achieving a lower root mean square error across all
parameters. The negligible computational cost and customization make C4PM a
valuable tool for wave data assimilation, improving the reliability of
forecasts and model validations.",2024-12-12 18:33:14+00:00,"['Andre Luiz Cordeiro dos Santos', 'Felipe Marques dos Santos', 'Nelson Violante-Carvalho', 'Luiz Mariano Carvalho', 'Helder Manoel Venceslau']",cross assignment directional wave spectra critical task wave datum assimilation traditionally method rely parameter spectral distance energy rank approach fail account complexity wave field lead inaccuracy address limitation propose controlled parameter method independently consider integrate wave parameter method enhance accuracy robustness cross assignment offer flexibility assign weight control wave parameter compare parameter spectral distance method datum buoy moor km apart deep water method produce negligible bias high correlation demonstrate superior performance prevent occurrence outlier achieve low root mean square error parameter negligible computational cost customization valuable tool wave datum assimilation improve reliability forecast model validation
Discretely self-similar exterior-naked singularities for the Einstein-scalar field system,"The problem of constructing naked singularities in general relativity can be
naturally divided into two parts: (i) the construction of the region exterior
to the past light cone of the singularity, extending all the way to (an
incomplete) future null infinity and yielding the nakedness property (what we
will call exterior-naked singularity regions); (ii) attaching an interior
fill-in that ensures that the singularity arises from regular initial data.
This problem has been resolved for the spherically symmetric Einstein-scalar
field system by Christodoulou, but his construction, based on a continuously
self-similar ansatz, requires that both the exterior and the interior regions
are mildly irregular on the past cone of the singularity. On the other hand,
numerical works suggest that there exist naked singularity spacetimes with
discrete self-similarity arising from smooth initial data. In this paper, we
revisit part (i) of the problem and we construct exterior-naked singularity
regions with discretely self-similar profiles which are smooth on the past cone
of the singularity. We show that the scalar field remains uniformly bounded,
but the singularity is characterized by the infinite oscillations of the scalar
field and the mass aspect ratio. (Our examples require however that the mass
aspect ratio is uniformly small, and thus the solutions are distinct from the
exterior regions of the numerical examples.) It remains an open problem to
smoothly attach interior fill-ins as in (ii) to our solutions, which would
yield a new construction of naked singularity spacetimes, now arising from
smooth initial data.",2024-12-12 18:30:25+00:00,"['Serban Cicortas', 'Christoph Kehle']",problem construct naked singularity general relativity naturally divide part construction region exterior past light cone singularity extend way incomplete future null infinity yield nakedness property exterior naked singularity region ii attach interior fill ensure singularity arise regular initial datum problem resolve spherically symmetric einstein scalar field system christodoulou construction base continuously self similar ansatz require exterior interior region mildly irregular past cone singularity hand numerical work suggest exist naked singularity spacetime discrete self similarity arise smooth initial datum paper revisit problem construct exterior naked singularity region discretely self similar profile smooth past cone singularity scalar field remain uniformly bound singularity characterize infinite oscillation scalar field mass aspect ratio example require mass aspect ratio uniformly small solution distinct exterior region numerical example remain open problem smoothly attach interior fill in ii solution yield new construction naked singularity spacetime arise smooth initial datum
Bayesian nonparametric mixtures of Archimedean copulas,"Copula-based dependence modelling often relies on parametric formulations.
This is mathematically convenient but can be statistically inefficient if the
parametric families are not suitable for the data and model in focus. To
improve the flexibility in modeling dependence, we consider a Bayesian
nonparametric mixture model of Archimedean copulas which can capture complex
dependence patterns and can be extended to arbitrary dimensions. In particular
we use the Poisson-Dirichlet process as mixing distribution over the single
parameter of the Archimedean copulas. Properties of the mixture model are
studied for the main Archimedenan families and posterior distributions are
sampled via their full conditional distributions. Performance of the model is
via numerical experiments involving simulated and real data.",2024-12-12 18:29:12+00:00,"['Ruyi Pan', 'Luis E. Nieto-Barajas', 'Radu V. Craiu']",copula base dependence modelling rely parametric formulation mathematically convenient statistically inefficient parametric family suitable datum model focus improve flexibility model dependence consider bayesian nonparametric mixture model archimedean copula capture complex dependence pattern extend arbitrary dimension particular use poisson dirichlet process mix distribution single parameter archimedean copula property mixture model study main archimedenan family posterior distribution sample conditional distribution performance model numerical experiment involve simulated real datum
Capturing the Temporal Dependence of Training Data Influence,"Traditional data influence estimation methods, like influence function,
assume that learning algorithms are permutation-invariant with respect to
training data. However, modern training paradigms, especially for foundation
models using stochastic algorithms and multi-stage curricula, are sensitive to
data ordering, thus violating this assumption. This mismatch renders influence
functions inadequate for answering a critical question in machine learning: How
can we capture the dependence of data influence on the optimization trajectory
during training? To address this gap, we formalize the concept of
trajectory-specific leave-one-out (LOO) influence, which quantifies the impact
of removing a data point from a specific iteration during training, accounting
for the exact sequence of data encountered and the model's optimization
trajectory. However, exactly evaluating the trajectory-specific LOO presents a
significant computational challenge. To address this, we propose data value
embedding, a novel technique enabling efficient approximation of
trajectory-specific LOO. Specifically, we compute a training data embedding
that encapsulates the cumulative interactions between data and the evolving
model parameters. The LOO can then be efficiently approximated through a simple
dot-product between the data value embedding and the gradient of the given test
data. As data value embedding captures training data ordering, it offers
valuable insights into model training dynamics. In particular, we uncover
distinct phases of data influence, revealing that data points in the early and
late stages of training exert a greater impact on the final model. These
insights translate into actionable strategies for managing the computational
overhead of data selection by strategically timing the selection process,
potentially opening new avenues in data curation research.",2024-12-12 18:28:55+00:00,"['Jiachen T. Wang', 'Dawn Song', 'James Zou', 'Prateek Mittal', 'Ruoxi Jia']",traditional data influence estimation method like influence function assume learn algorithm permutation invariant respect training datum modern training paradigm especially foundation model stochastic algorithm multi stage curricula sensitive datum ordering violate assumption mismatch render influence function inadequate answer critical question machine learning capture dependence datum influence optimization trajectory training address gap formalize concept trajectory specific leave loo influence quantify impact remove data point specific iteration training account exact sequence datum encounter model optimization trajectory exactly evaluate trajectory specific loo present significant computational challenge address propose datum value embed novel technique enable efficient approximation trajectory specific loo specifically compute training datum embed encapsulate cumulative interaction datum evolve model parameter loo efficiently approximate simple dot product data value embed gradient give test datum datum value embed capture train datum order offer valuable insight model training dynamic particular uncover distinct phase datum influence reveal data point early late stage training exert great impact final model insight translate actionable strategy manage computational overhead data selection strategically time selection process potentially open new avenue data curation research
Identification of structures driving trailing-edge noise. Part I -- Experimental investigation,"Trailing-edge (TE) noise is the main contributor to the acoustic signature of
flows over airfoils. It originates from the interaction of turbulent structures
in the airfoil boundary layer with the TE. This study experimentally identifies
the flow structures responsible for TE noise by decomposing the data into
spanwise modes and examining the impact of spanwise coherent structures on
sound emission. We analyse a NACA0012 airfoil at moderate Reynolds numbers,
ensuring broadband TE noise, and use synchronous measurements of surface and
far-field acoustic pressure fluctuations with custom spanwise microphone
arrays. Our results demonstrate the key role of coherent structures with large
spanwise wavelengths in generating broadband TE noise. Spanwise modal
decomposition of the acoustic field shows that only waves with spanwise
wavenumbers below the acoustic wavenumber contribute to the radiated acoustic
spectrum, consistent with theoretical scattering conditions. Moreover, a strong
correlation is found between spanwise-coherent (zero wavenumber) flow
structures and radiated acoustics. At frequencies corresponding to peak TE
noise emission, the turbulent structures responsible for radiation exhibit
strikingly large spanwise wavelengths, exceeding $60\%$ of the airfoil chord
length. These findings have implications for numerical and experimental TE
noise analysis and flow control. The correlation between spectrally decomposed
turbulent fluctuations and TE noise paves the way for future aeroacoustic
modelling through linearized mean field analysis. A companion paper further
explores the nature of the spanwise-coherent structures using high-resolution
numerical simulations of the same setup.",2024-12-12 18:27:53+00:00,"['Simon Demange', 'Zhenyang Yuan', 'Simon Jekosch', 'Ennes Sarradj', 'Ardeshir Hanifi', 'André V. G. Cavalieri', 'Kilian Oberleithner']",trail edge te noise main contributor acoustic signature flow airfoil originate interaction turbulent structure airfoil boundary layer te study experimentally identify flow structure responsible te noise decompose datum spanwise mode examine impact spanwise coherent structure sound emission analyse airfoil moderate reynolds number ensure broadband te noise use synchronous measurement surface far field acoustic pressure fluctuation custom spanwise microphone array result demonstrate key role coherent structure large spanwise wavelength generate broadband te noise spanwise modal decomposition acoustic field show wave spanwise wavenumber acoustic wavenumber contribute radiate acoustic spectrum consistent theoretical scattering condition strong correlation find spanwise coherent zero wavenumber flow structure radiate acoustic frequency correspond peak te noise emission turbulent structure responsible radiation exhibit strikingly large spanwise wavelength exceed airfoil chord length finding implication numerical experimental te noise analysis flow control correlation spectrally decompose turbulent fluctuation te noise pave way future aeroacoustic modelling linearize mean field analysis companion paper explore nature spanwise coherent structure high resolution numerical simulation setup
Defect density of states of tin oxide and copper oxide p-type thin-film transistors,"The complete subgap defect density of states (DoS) is measured using the
ultrabroadband (0.15 to 3.5 eV) photoconduction response from p-type thin-film
transistors (TFTs) of tin oxide, SnO, and copper oxide, Cu$_2$O. The TFT
photoconduction spectra clearly resolve all bandgaps that further show the
presence of interfacial and oxidized minority phases. In tin oxide, the SnO
majority phase has a small 0.68 eV bandgap enabling ambipolar or p-mode TFT
operation. By contrast, in copper oxide TFTs, an oxidized minority phase with a
1.4 eV bandgap corresponding to CuO greatly reduces the channel hole mobility
at the charge accumulation region. Three distinct subgap DoS peaks are resolved
for the copper oxide TFT and are best ascribed to copper vacancies,
oxygen-on-copper antisites, and oxygen interstitials. For tin oxide TFTs, five
subgap DoS peaks are observed and are similarly linked to tin vacancies, oxygen
vacancies, and oxygen interstitials. Unipolar p-type TFT is achieved in tin
oxide only when the conduction band-edge defect density peak ascribed to oxygen
interstitials is large enough to suppress any n-mode conduction. Near the
valence band edge in both active channel materials, the metal vacancy peak
densities determine the hole concentrations, which further simulate the
observed TFT threshold voltages.",2024-12-12 18:22:51+00:00,"['Måns J. Mattsson', 'Kham M. Niang', 'Jared Parker', 'David J. Meeth', 'John F. Wager', 'Andrew J. Flewitt', 'Matt W. Graham']",complete subgap defect density state dos measure ultrabroadband ev photoconduction response p type thin film transistor tfts tin oxide sno copper oxide tft photoconduction spectra clearly resolve bandgap presence interfacial oxidize minority phase tin oxide sno majority phase small ev bandgap enable ambipolar p mode tft operation contrast copper oxide tfts oxidize minority phase ev bandgap correspond cuo greatly reduce channel hole mobility charge accumulation region distinct subgap dos peak resolve copper oxide tft well ascribe copper vacancy oxygen copper antisite oxygen interstitial tin oxide tfts subgap dos peak observe similarly link tin vacancy oxygen vacancy oxygen interstitial unipolar p type tft achieve tin oxide conduction band edge defect density peak ascribe oxygen interstitial large suppress n mode conduction near valence band edge active channel material metal vacancy peak density determine hole concentration simulate observe tft threshold voltage
Can Modern LLMs Act as Agent Cores in Radiology~Environments?,"Advancements in large language models (LLMs) have paved the way for LLM-based
agent systems that offer enhanced accuracy and interpretability across various
domains. Radiology, with its complex analytical requirements, is an ideal field
for the application of these agents. This paper aims to investigate the
pre-requisite question for building concrete radiology agents which is, `Can
modern LLMs act as agent cores in radiology environments?' To investigate it,
we introduce RadABench with three-fold contributions: First, we present
RadABench-Data, a comprehensive synthetic evaluation dataset for LLM-based
agents, generated from an extensive taxonomy encompassing 6 anatomies, 5
imaging modalities, 10 tool categories, and 11 radiology tasks. Second, we
propose RadABench-EvalPlat, a novel evaluation platform for agents featuring a
prompt-driven workflow and the capability to simulate a wide range of radiology
toolsets. Third, we assess the performance of 7 leading LLMs on our benchmark
from 5 perspectives with multiple metrics. Our findings indicate that while
current LLMs demonstrate strong capabilities in many areas, they are still not
sufficiently advanced to serve as the central agent core in a fully operational
radiology agent system. Additionally, we identify key factors influencing the
performance of LLM-based agent cores, offering insights for clinicians on how
to apply agent systems in real-world radiology practices effectively. All of
our code and data are open-sourced in
https://github.com/MAGIC-AI4Med/RadABench.",2024-12-12 18:20:16+00:00,"['Qiaoyu Zheng', 'Chaoyi Wu', 'Pengcheng Qiu', 'Lisong Dai', 'Ya Zhang', 'Yanfeng Wang', 'Weidi Xie']",advancement large language model llms pave way llm base agent system offer enhanced accuracy interpretability domain radiology complex analytical requirement ideal field application agent paper aim investigate pre requisite question build concrete radiology agent modern llms act agent core radiology environment investigate introduce radabench fold contribution present radabench data comprehensive synthetic evaluation dataset llm base agent generate extensive taxonomy encompass anatomy imaging modality tool category radiology task second propose radabench evalplat novel evaluation platform agent feature prompt drive workflow capability simulate wide range radiology toolset assess performance lead llm benchmark perspective multiple metric finding indicate current llms demonstrate strong capability area sufficiently advanced serve central agent core fully operational radiology agent system additionally identify key factor influence performance llm base agent core offer insight clinician apply agent system real world radiology practice effectively code datum open sourced
GainAdaptor: Learning Quadrupedal Locomotion with Dual Actors for Adaptable and Energy-Efficient Walking on Various Terrains,"Deep reinforcement learning (DRL) has emerged as an innovative solution for
controlling legged robots in challenging environments using minimalist
architectures. Traditional control methods for legged robots, such as inverse
dynamics, either directly manage joint torques or use proportional-derivative
(PD) controllers to regulate joint positions at a higher level. In case of DRL,
direct torque control presents significant challenges, leading to a preference
for joint position control. However, this approach necessitates careful
adjustment of joint PD gains, which can limit both adaptability and efficiency.
In this paper, we propose GainAdaptor, an adaptive gain control framework that
autonomously tunes joint PD gains to enhance terrain adaptability and energy
efficiency. The framework employs a dual-actor algorithm to dynamically adjust
the PD gains based on varying ground conditions. By utilizing a divided action
space, GainAdaptor efficiently learns stable and energy-efficient locomotion.
We validate the effectiveness of the proposed method through experiments
conducted on a Unitree Go1 robot, demonstrating improved locomotion performance
across diverse terrains.",2024-12-12 18:06:22+00:00,"['Mincheol Kim', 'Nahyun Kwon', 'Jung-Yup Kim']",deep reinforcement learning drl emerge innovative solution control legged robot challenge environment minimalist architecture traditional control method legged robot inverse dynamic directly manage joint torque use proportional derivative pd controller regulate joint position high level case drl direct torque control present significant challenge lead preference joint position control approach necessitate careful adjustment joint pd gain limit adaptability efficiency paper propose gainadaptor adaptive gain control framework autonomously tune joint pd gain enhance terrain adaptability energy efficiency framework employ dual actor algorithm dynamically adjust pd gain base vary ground condition utilize divided action space gainadaptor efficiently learn stable energy efficient locomotion validate effectiveness propose method experiment conduct unitree robot demonstrate improve locomotion performance diverse terrain
Clifford Perturbation Approximation for Quantum Error Mitigation,"Quantum error mitigation (QEM) is critical for harnessing the potential of
near-term quantum devices. Particularly, QEM protocols can be designed based on
machine learning, where the mapping between noisy computational outputs and
ideal ones can be learned on a training set consisting of Clifford circuits or
near-Clifford circuits that contain only a limited number of non-Clifford
gates. This learned mapping is then applied to noisy target circuits to
estimate the ideal computational output. In this work, we propose a
learning-based error mitigation framework called Clifford Perturbation Data
Regression (CPDR), which constructs training sets by Clifford circuits with
small perturbations. Specifically, these circuits are parameterized quantum
circuits, where the rotation angles of the gates are restricted to a narrow
range, ensuring that the gates remain close to Clifford gates. This design
enables the efficient simulation of the training circuits using the Sparse
Pauli Dynamics method. As a result, CPDR is able to utilize training sets with
a better diversity to train the model, compared with previous learning-based
QEM models that construct training sets with only Clifford or near-Clifford
circuits. Numerical simulations on small-scale Ising model circuits demonstrate
that the performance of CPDR dramatically outperforms that of existing methods
such as Zero-Noise Extrapolation and learning-based Probabilistic Error
Cancellation. Furthermore, using the experimental data from IBM's 127-qubit
Eagle processor, our findings suggest that CPDR demonstrates improved accuracy
compared to the original mitigation results reported in [Nature 618, 500
(2023)].",2024-12-12 18:01:54+00:00,"['Ruiqi Zhang', 'Yuguo Shao', 'Fuchuan Wei', 'Song Cheng', 'Zhaohui Wei', 'Zhengwei Liu']",quantum error mitigation qem critical harness potential near term quantum device particularly qem protocol design base machine learning mapping noisy computational output ideal one learn training set consist clifford circuit near clifford circuit contain limited number non clifford gate learn mapping apply noisy target circuit estimate ideal computational output work propose learning base error mitigation framework call clifford perturbation data regression cpdr construct training set clifford circuit small perturbation specifically circuit parameterized quantum circuit rotation angle gate restrict narrow range ensure gate remain close clifford gate design enable efficient simulation training circuit sparse pauli dynamics method result cpdr able utilize training set well diversity train model compare previous learning base qem model construct training set clifford near clifford circuit numerical simulation small scale ising model circuit demonstrate performance cpdr dramatically outperform exist method zero noise extrapolation learning base probabilistic error cancellation furthermore experimental datum ibm qubit eagle processor finding suggest cpdr demonstrate improved accuracy compare original mitigation result report nature
Geometric Deep Learning for Realized Covariance Matrix Forecasting,"Traditional methods employed in matrix volatility forecasting often overlook
the inherent Riemannian manifold structure of symmetric positive definite
matrices, treating them as elements of Euclidean space, which can lead to
suboptimal predictive performance. Moreover, they often struggle to handle
high-dimensional matrices. In this paper, we propose a novel approach for
forecasting realized covariance matrices of asset returns using a
Riemannian-geometry-aware deep learning framework. In this way, we account for
the geometric properties of the covariance matrices, including possible
non-linear dynamics and efficient handling of high-dimensionality. Moreover,
building upon a Fr\'echet sample mean of realized covariance matrices, we are
able to extend the HAR model to the matrix-variate. We demonstrate the efficacy
of our approach using daily realized covariance matrices for the 50 most
capitalized companies in the S&P 500 index, showing that our method outperforms
traditional approaches in terms of predictive accuracy.",2024-12-12 18:01:48+00:00,"['Andrea Bucci', 'Michele Palma', 'Chao Zhang']",traditional method employ matrix volatility forecasting overlook inherent riemannian manifold structure symmetric positive definite matrix treat element euclidean space lead suboptimal predictive performance struggle handle high dimensional matrix paper propose novel approach forecasting realize covariance matrix asset return riemannian geometry aware deep learning framework way account geometric property covariance matrix include possible non linear dynamic efficient handling high dimensionality build sample mean realize covariance matrix able extend har model matrix variate demonstrate efficacy approach daily realize covariance matrix capitalize company index show method outperform traditional approach term predictive accuracy
Search for Inelastic Boosted Dark Matter with the ICARUS Detector at the Gran Sasso Underground National Laboratory,"We present the result of a search for inelastic boosted dark matter using the
data corresponding to an exposure of 0.13 kton$\cdot$year, collected by the
ICARUS T-600 detector during its 2012--2013 operational period at the INFN Gran
Sasso Underground National Laboratory. The benchmark boosted dark matter model
features a multi-particle dark sector with a U(1)$'$ gauge boson, the dark
photon. The kinetic mixing of the dark photon with the Standard Model photon
allows for a portal between the dark sector and the visible sector. The
inelastic boosted dark matter interaction occurs when a dark matter particle
inelastically scatters with an electron in the ICARUS detector, producing an
outgoing, heavier dark sector state which subsequently decays back down to the
dark matter particle, emitting a dark photon. The dark photon subsequently
couples to a Standard Model photon through kinetic mixing. The Standard Model
photon then converts to an electron-positron pair in the detector. This
interaction process provides a distinct experimental signature which consists
of a recoil electron from the primary interaction and an associated
electron-positron pair from the secondary vertex. After analyzing 4,134
triggered events, the search results in zero observed events. Exclusion limits
are set in the dark photon mass and coupling ($m_X, \epsilon$) parameter space
for several selected optimal boosted dark matter mass sets.",2024-12-12 18:00:30+00:00,"['H. Carranza', 'J. Yu', 'B. Brown', 'S. Blanchard', 'S. Chakraborty', 'R. Raut', 'D. Kim', 'M. Antonello', 'B. Baibussinov', 'V. Bellini', 'P. Benetti', 'F. Boffelli', '6 M. Bonesini', 'A. Bubak', 'E. Calligarich', 'S. Centro', 'A. Cesana', 'K. Cieslik', 'A. G. Cocco', 'A. Dabrowska', 'A. Dermenev', 'A. Falcone', 'C. Farnese', 'A. Fava', 'A. Ferrari', 'D. Gibin', 'S. Gninenko', 'A. Guglielmi', 'J. Holeczek', 'M. Janik', 'M. Kirsanov', 'J. Kisiel', 'I. Kochanek', 'J. Lagoda', 'A. Menegolli', 'G. Meng', 'C. Montanari', 'S. Otwinowski', 'C. Petta', 'F. Pietropaolo', 'A. Rappoldi', 'G. L. Raselli', 'M. Rossella', 'C. Rubbia', 'P. Sala', 'A. Scaramelli', 'F. Sergiampietri', 'D. Stefan', 'M. Szarska', 'M. Terrani', 'M. Torti', 'F. Tortorici', 'F. Varanini', 'S. Ventura', 'C. Vignoli', 'H. Wang', 'X. Yang', 'A. Zalewska', 'A. Zani', 'K. Zaremba']",present result search inelastic boost dark matter datum correspond exposure collect icarus detector operational period infn gran sasso underground national laboratory benchmark boost dark matter model feature multi particle dark sector gauge boson dark photon kinetic mixing dark photon standard model photon allow portal dark sector visible sector inelastic boost dark matter interaction occur dark matter particle inelastically scatter electron icarus detector produce outgoing heavy dark sector state subsequently decay dark matter particle emit dark photon dark photon subsequently couple standard model photon kinetic mixing standard model photon convert electron positron pair detector interaction process provide distinct experimental signature consist recoil electron primary interaction associated electron positron pair secondary vertex analyze trigger event search result zero observed event exclusion limit set dark photon mass coupling parameter space select optimal boost dark matter mass set
Influence of Nb Alloying on Nb Recrystallization and the Upper Critical Field of Nb3Sn,"Nb3Sn conductors are important candidates for high-field magnets for particle
accelerators, and they continue to be widely used for many laboratory and NMR
magnets. However, the critical current density, Jc, of present Nb3Sn conductors
declines swiftly above 12-15T. State-of-the-art Ta- and Ti-doped strands
exhibit upper critical field, Hc2, values of 24-26.5T (4.2K) and do not reach
the FCC target Jc, which serves as the present stretch target for Nb3Sn
development. As recently demonstrated, to meet this goal requires enhanced
vortex pinning but an independent and supplementary approach is to
significantly enhance Hc2. In this study, we have arc-melted multiple Nb alloys
with added Hf, Zr, Ta and Ti and drawn them successfully into monofilament
wires to investigate the possibilities of Hc2 enhancement through alloying.
Hc2(T) was measured for all samples in fields up to 16T and some up to 31T. We
have found that all alloys show good agreement with the standard Werthamer,
Helfand, and Hohenberg (WHH) fitting procedure without the need to adjust the
paramagnetic limitation parameter (alpha) and spin-orbit scattering parameter
(lambda_so). The evaluation of dHc2/dT near Tc, which is proportional to the
electronic specific heat coefficient (gamma) and the normal state resistivity
(rho_n), allows a better understanding of the induced disorder introduced by
alloying in the A15 phase. So far, we have observed that Hf alloying of pure Nb
can enhance Hc2(0) by 3-4T to 28T, while adding just 1 at. %Hf or Zr into a
Nb4Ta base alloy can raise Hc2(0) to 31T. Very importantly we find that Hf and
Zr raise the alloy recrystallization temperature above the usual A15 reaction
temperature range of 650{\deg}C-750{\deg}C, thus ensuring denser A15 phase
nucleation in the Nb alloy grain boundaries, possibly leading to a more
homogeneous A15 phase Sn content and refined A15 grain size. The potential
for...",2024-12-12 17:59:30+00:00,"['Nawaraj Paudel', 'Chiara Tarantini', 'Shreyas Balachandran', 'William L. Starch', 'Peter J. Lee', 'David C. Larbalestier']",conductor important candidate high field magnet particle accelerator continue widely laboratory nmr magnet critical current density jc present conductor decline swiftly state art ti dope strand exhibit upper critical field value t k reach fcc target jc serve present stretch target development recently demonstrate meet goal require enhance vortex pin independent supplementary approach significantly enhance study arc melt multiple nb alloy add hf zr ta ti draw successfully monofilament wire investigate possibility enhancement alloying measure sample field t find alloy good agreement standard werthamer helfand hohenberg whh fitting procedure need adjust paramagnetic limitation parameter alpha spin orbit scatter parameter evaluation dt near tc proportional electronic specific heat coefficient gamma normal state resistivity allow well understanding induced disorder introduce alloying phase far observe hf alloying pure nb enhance t t add hf zr base alloy raise importantly find hf zr raise alloy recrystallization temperature usual reaction temperature range ensure denser phase nucleation nb alloy grain boundary possibly lead homogeneous phase sn content refine grain size potential
GEAL: Generalizable 3D Affordance Learning with Cross-Modal Consistency,"Identifying affordance regions on 3D objects from semantic cues is essential
for robotics and human-machine interaction. However, existing 3D affordance
learning methods struggle with generalization and robustness due to limited
annotated data and a reliance on 3D backbones focused on geometric encoding,
which often lack resilience to real-world noise and data corruption. We propose
GEAL, a novel framework designed to enhance the generalization and robustness
of 3D affordance learning by leveraging large-scale pre-trained 2D models. We
employ a dual-branch architecture with Gaussian splatting to establish
consistent mappings between 3D point clouds and 2D representations, enabling
realistic 2D renderings from sparse point clouds. A granularity-adaptive fusion
module and a 2D-3D consistency alignment module further strengthen cross-modal
alignment and knowledge transfer, allowing the 3D branch to benefit from the
rich semantics and generalization capacity of 2D models. To holistically assess
the robustness, we introduce two new corruption-based benchmarks: PIAD-C and
LASO-C. Extensive experiments on public datasets and our benchmarks show that
GEAL consistently outperforms existing methods across seen and novel object
categories, as well as corrupted data, demonstrating robust and adaptable
affordance prediction under diverse conditions. Code and corruption datasets
have been made publicly available.",2024-12-12 17:59:03+00:00,"['Dongyue Lu', 'Lingdong Kong', 'Tianxin Huang', 'Gim Hee Lee']",identify affordance region object semantic cue essential robotic human machine interaction exist affordance learning method struggle generalization robustness limited annotate datum reliance backbone focus geometric encoding lack resilience real world noise datum corruption propose geal novel framework design enhance generalization robustness affordance learn leverage large scale pre train model employ dual branch architecture gaussian splatte establish consistent mapping point cloud representation enable realistic rendering sparse point cloud granularity adaptive fusion module consistency alignment module strengthen cross modal alignment knowledge transfer allow branch benefit rich semantic generalization capacity model holistically assess robustness introduce new corruption base benchmark piad c laso extensive experiment public dataset benchmark geal consistently outperform exist method see novel object category corrupted datum demonstrate robust adaptable affordance prediction diverse condition code corruption dataset publicly available
Vision Transformers for Efficient Indoor Pathloss Radio Map Prediction,"Vision Transformers (ViTs) have demonstrated remarkable success in achieving
state-of-the-art performance across various image-based tasks and beyond. In
this study, we employ a ViT-based neural network to address the problem of
indoor pathloss radio map prediction. The network's generalization ability is
evaluated across diverse settings, including unseen buildings, frequencies, and
antennas with varying radiation patterns. By leveraging extensive data
augmentation techniques and pretrained DINOv2 weights, we achieve promising
results, even under the most challenging scenarios.",2024-12-12 17:55:00+00:00,"['Edvard Ghukasyan', 'Hrant Khachatrian', 'Rafayel Mkrtchyan', 'Theofanis P. Raptis']",vision transformers vits demonstrate remarkable success achieve state art performance image base task study employ vit base neural network address problem indoor pathloss radio map prediction network generalization ability evaluate diverse setting include unseen building frequency antenna varying radiation pattern leverage extensive datum augmentation technique pretraine weight achieve promising result challenging scenario
High Precision Binding Energies from Physics Informed Machine Learning,"Twelve physics informed machine learning models have been trained to model
binding energy residuals. Our approach begins with determining the difference
between measured experimental binding energies and three different mass models.
Then four machine learning approaches are used to train on each energy
difference. The most successful ML technique both in interpolation and
extrapolation is the least squares boosted ensemble of trees. The best model
resulting from that technique utilizes eight physical features to model the
difference between experimental atomic binding energy values in AME 2012 and
the Duflo Zucker mass model. This resulted in a model that fit the training
data with a standard deviation of 17 keV and that has a standard deviation of
92 keV when compared all of the values in the AME 2020. The extrapolation
capability of each model is discussed and the accuracy of predicting new mass
measurements has also been tested.",2024-12-12 17:54:34+00:00,"['Ian Bentley', 'James Tedder', 'Marwan Gebran', 'Ayan Paul']",physics inform machine learning model train model bind energy residual approach begin determine difference measured experimental bind energy different mass model machine learning approach train energy difference successful ml technique interpolation extrapolation square boost ensemble tree good model result technique utilize physical feature model difference experimental atomic bind energy value ame duflo zucker mass model result model fit training datum standard deviation kev standard deviation kev compare value ame extrapolation capability model discuss accuracy predict new mass measurement test
AdS amplitudes as CFT correlators,"We show that AdS amplitudes are CFT correlators to all orders in the loop
expansion by showing that they obey the conformal Ward identities. In
particular, we provide explicit formulas for the constants and functions of
cross-ratios that determine the CFT correlators in terms of bulk data.",2024-12-12 17:53:07+00:00,"['Maximo Bañados', 'Ernesto Bianchi', 'Ivan Muñoz', 'Kostas Skenderis']",ads amplitude cft correlator order loop expansion show obey conformal ward identity particular provide explicit formula constant function cros ratio determine cft correlator term bulk datum
Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition,"As Multi-modal Large Language Models (MLLMs) evolve, expanding beyond
single-domain capabilities is essential to meet the demands for more versatile
and efficient AI. However, previous omni-models have insufficiently explored
speech, neglecting its integration with multi-modality. We introduce Lyra, an
efficient MLLM that enhances multimodal abilities, including advanced
long-speech comprehension, sound understanding, cross-modality efficiency, and
seamless speech interaction. To achieve efficiency and speech-centric
capabilities, Lyra employs three strategies: (1) leveraging existing
open-source large models and a proposed multi-modality LoRA to reduce training
costs and data requirements; (2) using a latent multi-modality regularizer and
extractor to strengthen the relationship between speech and other modalities,
thereby enhancing model performance; and (3) constructing a high-quality,
extensive dataset that includes 1.5M multi-modal (language, vision, audio) data
samples and 12K long speech samples, enabling Lyra to handle complex long
speech inputs and achieve more robust omni-cognition. Compared to other
omni-methods, Lyra achieves state-of-the-art performance on various
vision-language, vision-speech, and speech-language benchmarks, while also
using fewer computational resources and less training data.",2024-12-12 17:50:39+00:00,"['Zhisheng Zhong', 'Chengyao Wang', 'Yuqi Liu', 'Senqiao Yang', 'Longxiang Tang', 'Yuechen Zhang', 'Jingyao Li', 'Tianyuan Qu', 'Yanwei Li', 'Yukang Chen', 'Shaozuo Yu', 'Sitong Wu', 'Eric Lo', 'Shu Liu', 'Jiaya Jia']",multi modal large language models mllm evolve expand single domain capability essential meet demand versatile efficient ai previous omni model insufficiently explore speech neglect integration multi modality introduce lyra efficient mllm enhance multimodal ability include advanced long speech comprehension sound understanding cross modality efficiency seamless speech interaction achieve efficiency speech centric capability lyra employ strategy leverage exist open source large model propose multi modality lora reduce training cost datum requirement latent multi modality regularizer extractor strengthen relationship speech modality enhance model performance construct high quality extensive dataset include m multi modal language vision audio datum sample k long speech sample enable lyra handle complex long speech input achieve robust omni cognition compare omni method lyra achieve state art performance vision language vision speech speech language benchmark few computational resource training datum
Loss function to optimise signal significance in particle physics,"We construct a surrogate loss to directly optimise the significance metric
used in particle physics. We evaluate our loss function for a simple event
classification task using a linear model and show that it produces decision
boundaries that change according to the cross sections of the processes
involved. We find that the models trained with the new loss have higher signal
efficiency for similar values of estimated signal significance compared to ones
trained with a cross-entropy loss, showing promise to improve sensitivity of
particle physics searches at colliders.",2024-12-12 17:48:57+00:00,"['Jai Bardhan', 'Cyrin Neeraj', 'Subhadip Mitra', 'Tanumoy Mandal']",construct surrogate loss directly optimise significance metric particle physic evaluate loss function simple event classification task linear model produce decision boundary change accord cross section process involve find model train new loss high signal efficiency similar value estimate signal significance compare one train cross entropy loss show promise improve sensitivity particle physics search collider
A novel ML-fuzzy control system for optimizing PHEV fuel efficiency and extending electric range under diverse driving conditions,"Aiming for a greener transportation future, this study introduces an
innovative control system for plug-in hybrid electric vehicles (PHEVs) that
utilizes machine learning (ML) techniques to forecast energy usage in the pure
electric mode of the vehicle and optimize power allocation across different
operational modes, including pure electric, series hybrid, parallel hybrid, and
internal combustion operation. The fuzzy logic decision-making process governs
the vehicle control system. The performance was assessed under various driving
conditions. Key findings include a significant enhancement in pure electric
mode efficiency, achieving an extended full-electric range of approximately 84
kilometers on an 80% utilization of a 20-kWh battery pack. During the WLTC
driving cycle, the control system reduced fuel consumption to 2.86 L/100km,
representing a 20% reduction in gasoline-equivalent fuel consumption.
Evaluations of vehicle performance at discrete driving speeds, highlighted
effective energy management, with the vehicle battery charging at lower speeds
and discharging at higher speeds, showing optimized energy recovery and
consumption strategies. Initial battery charge levels notably influenced
vehicle performance. A 90% initial charge enabled prolonged all-electric
operation, minimizing fuel consumption to 2 L/100km less than that of the base
control system. Real-world driving pattern analysis revealed significant
variations, with shorter, slower cycles requiring lower fuel consumption due to
prioritized electric propulsion, while longer, faster cycles increased internal
combustion engine usage. The control system also adapted to different battery
state of health (SOH) conditions, with higher SOH facilitating extended
electric mode usage, reducing total fuel consumption by up to 2.87 L/100km.",2024-12-12 17:47:19+00:00,"['Mehrdad Raeesi', 'Saba Mansour', 'Sina Changizian']",aim greener transportation future study introduce innovative control system plug hybrid electric vehicle phev utilize machine learning ml technique forecast energy usage pure electric mode vehicle optimize power allocation different operational mode include pure electric series hybrid parallel hybrid internal combustion operation fuzzy logic decision make process govern vehicle control system performance assess driving condition key finding include significant enhancement pure electric mode efficiency achieve extend electric range approximately kilometer utilization kwh battery pack wltc drive cycle control system reduce fuel consumption km represent reduction gasoline equivalent fuel consumption evaluation vehicle performance discrete driving speed highlight effective energy management vehicle battery charge low speed discharge high speed show optimize energy recovery consumption strategy initial battery charge level notably influence vehicle performance initial charge enable prolong electric operation minimize fuel consumption km base control system real world driving pattern analysis reveal significant variation short slow cycle require low fuel consumption prioritize electric propulsion long fast cycle increase internal combustion engine usage control system adapt different battery state health soh condition high soh facilitating extend electric mode usage reduce total fuel consumption km
Gradient descent inference in empirical risk minimization,"Gradient descent is one of the most widely used iterative algorithms in
modern statistical learning. However, its precise algorithmic dynamics in
high-dimensional settings remain only partially understood, which has therefore
limited its broader potential for statistical inference applications.
  This paper provides a precise, non-asymptotic distributional characterization
of gradient descent iterates in a broad class of empirical risk minimization
problems, in the so-called mean-field regime where the sample size is
proportional to the signal dimension. Our non-asymptotic state evolution theory
holds for both general non-convex loss functions and non-Gaussian data, and
reveals the central role of two Onsager correction matrices that precisely
characterize the non-trivial dependence among all gradient descent iterates in
the mean-field regime.
  Although the Onsager correction matrices are typically analytically
intractable, our state evolution theory facilitates a generic gradient descent
inference algorithm that consistently estimates these matrices across a broad
class of models. Leveraging this algorithm, we show that the state evolution
can be inverted to construct (i) data-driven estimators for the generalization
error of gradient descent iterates and (ii) debiased gradient descent iterates
for inference of the unknown signal. Detailed applications to two canonical
models--linear regression and (generalized) logistic regression--are worked out
to illustrate model-specific features of our general theory and inference
methods.",2024-12-12 17:47:08+00:00,"['Qiyang Han', 'Xiaocong Xu']",gradient descent widely iterative algorithm modern statistical learning precise algorithmic dynamic high dimensional setting remain partially understand limit broad potential statistical inference application paper provide precise non asymptotic distributional characterization gradient descent iterate broad class empirical risk minimization problem call mean field regime sample size proportional signal dimension non asymptotic state evolution theory hold general non convex loss function non gaussian datum reveal central role onsager correction matrice precisely characterize non trivial dependence gradient descent iterate mean field regime onsager correction matrix typically analytically intractable state evolution theory facilitate generic gradient descent inference algorithm consistently estimate matrix broad class model leverage algorithm state evolution invert construct data drive estimator generalization error gradient descent iterate ii debiase gradient descent iterate inference unknown signal detailed application canonical model linear regression generalize logistic regression work illustrate model specific feature general theory inference method
Assessing the Role of Volumetric Brain Information in Multiple Sclerosis Progression,"Multiple sclerosis is a chronic autoimmune disease that affects the central
nervous system. Understanding multiple sclerosis progression and identifying
the implicated brain structures is crucial for personalized treatment
decisions. Deformation-based morphometry utilizes anatomical magnetic resonance
imaging to quantitatively assess volumetric brain changes at the voxel level,
providing insight into how each brain region contributes to clinical
progression with regards to neurodegeneration. Utilizing such voxel-level data
from a relapsing multiple sclerosis clinical trial, we extend a model-agnostic
feature importance metric to identify a robust and predictive feature set that
corresponds to clinical progression. These features correspond to brain regions
that are clinically meaningful in MS disease research, demonstrating their
scientific relevance. When used to predict progression using classical survival
models and 3D convolutional neural networks, the identified regions led to the
best-performing models, demonstrating their prognostic strength. We also find
that these features generalize well to other definitions of clinical
progression and can compensate for the omission of highly prognostic clinical
features, underscoring the predictive power and clinical relevance of
deformation-based morphometry as a regional identification tool.",2024-12-12 17:45:57+00:00,"['Andy A. Shen', 'Aidan McLoughlin', 'Zoe Vernon', 'Jonathan Lin', 'Richard A. D. Carano', 'Peter J. Bickel', 'Zhuang Song', 'Haiyan Huang']",multiple sclerosis chronic autoimmune disease affect central nervous system understand multiple sclerosis progression identify implicate brain structure crucial personalize treatment decision deformation base morphometry utilize anatomical magnetic resonance imaging quantitatively assess volumetric brain change voxel level provide insight brain region contribute clinical progression regard neurodegeneration utilize voxel level datum relapse multiple sclerosis clinical trial extend model agnostic feature importance metric identify robust predictive feature set correspond clinical progression feature correspond brain region clinically meaningful ms disease research demonstrate scientific relevance predict progression classical survival model convolutional neural network identify region lead well perform model demonstrate prognostic strength find feature generalize definition clinical progression compensate omission highly prognostic clinical feature underscore predictive power clinical relevance deformation base morphometry regional identification tool
iKap: Kinematics-aware Planning with Imperative Learning,"Trajectory planning in robotics aims to generate collision-free pose
sequences that can be reliably executed. Recently, vision-to-planning systems
have garnered increasing attention for their efficiency and ability to
interpret and adapt to surrounding environments. However, traditional modular
systems suffer from increased latency and error propagation, while purely
data-driven approaches often overlook the robot's kinematic constraints. This
oversight leads to discrepancies between planned trajectories and those that
are executable. To address these challenges, we propose iKap, a novel
vision-to-planning system that integrates the robot's kinematic model directly
into the learning pipeline. iKap employs a self-supervised learning approach
and incorporates the state transition model within a differentiable bi-level
optimization framework. This integration ensures the network learns
collision-free waypoints while satisfying kinematic constraints, enabling
gradient back-propagation for end-to-end training. Our experimental results
demonstrate that iKap achieves higher success rates and reduced latency
compared to the state-of-the-art methods. Besides the complete system, iKap
offers a visual-to-planning network that seamlessly integrates kinematics into
various controllers, providing a robust solution for robots navigating complex
and dynamic environments.",2024-12-12 17:43:58+00:00,"['Qihang Li', 'Zhuoqun Chen', 'Haoze Zheng', 'Haonan He', 'Shaoshu Su', 'Junyi Geng', 'Chen Wang']",trajectory planning robotic aim generate collision free pose sequence reliably execute recently vision planning system garner increase attention efficiency ability interpret adapt surround environment traditional modular system suffer increase latency error propagation purely data drive approach overlook robot kinematic constraint oversight lead discrepancy plan trajectory executable address challenge propose ikap novel vision planning system integrate robot kinematic model directly learn pipeline ikap employ self supervise learning approach incorporate state transition model differentiable bi level optimization framework integration ensure network learn collision free waypoint satisfy kinematic constraint enable gradient propagation end end training experimental result demonstrate ikap achieve high success rate reduced latency compare state art method complete system ikap offer visual planning network seamlessly integrate kinematic controller provide robust solution robot navigate complex dynamic environment
Regression and Classification with Single-Qubit Quantum Neural Networks,"Since classical machine learning has become a powerful tool for developing
data-driven algorithms, quantum machine learning is expected to similarly
impact the development of quantum algorithms. The literature reflects a
mutually beneficial relationship between machine learning and quantum
computing, where progress in one field frequently drives improvements in the
other. Motivated by the fertile connection between machine learning and quantum
computing enabled by parameterized quantum circuits, we use a
resource-efficient and scalable Single-Qubit Quantum Neural Network (SQQNN) for
both regression and classification tasks. The SQQNN leverages parameterized
single-qubit unitary operators and quantum measurements to achieve efficient
learning. To train the model, we use gradient descent for regression tasks. For
classification, we introduce a novel training method inspired by the Taylor
series, which can efficiently find a global minimum in a single step. This
approach significantly accelerates training compared to iterative methods.
Evaluated across various applications, the SQQNN exhibits virtually error-free
and strong performance in regression and classification tasks, including the
MNIST dataset. These results demonstrate the versatility, scalability, and
suitability of the SQQNN for deployment on near-term quantum devices.",2024-12-12 17:35:36+00:00,"['Leandro C. Souza', 'Bruno C. Guingo', 'Gilson Giraldi', 'Renato Portugal']",classical machine learning powerful tool develop data drive algorithm quantum machine learning expect similarly impact development quantum algorithm literature reflect mutually beneficial relationship machine learning quantum computing progress field frequently drive improvement motivate fertile connection machine learning quantum computing enable parameterized quantum circuit use resource efficient scalable single qubit quantum neural network sqqnn regression classification task sqqnn leverage parameterized single qubit unitary operator quantum measurement achieve efficient learning train model use gradient descent regression task classification introduce novel training method inspire taylor series efficiently find global minimum single step approach significantly accelerate training compare iterative method evaluate application sqqnn exhibit virtually error free strong performance regression classification task include mnist dataset result demonstrate versatility scalability suitability sqqnn deployment near term quantum device
Early Detection of At-Risk Students Using Machine Learning,"This research presents preliminary work to address the challenge of
identifying at-risk students using supervised machine learning and three unique
data categories: engagement, demographics, and performance data collected from
Fall 2023 using Canvas and the California State University, Fullerton
dashboard. We aim to tackle the persistent challenges of higher education
retention and student dropout rates by screening for at-risk students and
building a high-risk identification system. By focusing on previously
overlooked behavioral factors alongside traditional metrics, this work aims to
address educational gaps, enhance student outcomes, and significantly boost
student success across disciplines at the University. Pre-processing steps take
place to establish a target variable, anonymize student information, manage
missing data, and identify the most significant features. Given the mixed data
types in the datasets and the binary classification nature of this study, this
work considers several machine learning models, including Support Vector
Machines (SVM), Naive Bayes, K-nearest neighbors (KNN), Decision Trees,
Logistic Regression, and Random Forest. These models predict at-risk students
and identify critical periods of the semester when student performance is most
vulnerable. We will use validation techniques such as train test split and
k-fold cross-validation to ensure the reliability of the models. Our analysis
indicates that all algorithms generate an acceptable outcome for at-risk
student predictions, while Naive Bayes performs best overall.",2024-12-12 17:33:06+00:00,"['Azucena L. Jimenez Martinez', 'Kanika Sood', 'Rakeshkumar Mahto']",research present preliminary work address challenge identify risk student supervised machine learning unique datum category engagement demographic performance datum collect fall canvas california state university fullerton dashboard aim tackle persistent challenge high education retention student dropout rate screen risk student build high risk identification system focus previously overlook behavioral factor alongside traditional metric work aim address educational gap enhance student outcome significantly boost student success discipline university pre processing step place establish target variable anonymize student information manage missing datum identify significant feature give mixed datum type dataset binary classification nature study work consider machine learning model include support vector machines svm naive bayes k near neighbor knn decision trees logistic regression random forest model predict risk student identify critical period semester student performance vulnerable use validation technique train test split k fold cross validation ensure reliability model analysis indicate algorithm generate acceptable outcome risk student prediction naive bayes perform well overall
Inference under Staggered Adoption: Case Study of the Affordable Care Act,"Panel data consists of a collection of $N$ units that are observed over $T$
units of time. A policy or treatment is subject to staggered adoption if
different units take on treatment at different times and remains treated (or
never at all). Assessing the effectiveness of such a policy requires estimating
the treatment effect, corresponding to the difference between outcomes for
treated versus untreated units. We develop inference procedures that build upon
a computationally efficient matrix estimator for treatment effects in panel
data. Our routines return confidence intervals (CIs) both for individual
treatment effects, as well as for more general bilinear functionals of
treatment effects, with prescribed coverage guarantees. We apply these
inferential methods to analyze the effectiveness of Medicaid expansion portion
of the Affordable Care Act. Based on our analysis, Medicaid expansion has led
to substantial reductions in uninsurance rates, has reduced infant mortality
rates, and has had no significant effects on healthcare expenditures.",2024-12-12 17:32:35+00:00,"['Eric Xia', 'Yuling Yan', 'Martin J. Wainwright']",panel datum consist collection unit observe unit time policy treatment subject stagger adoption different unit treatment different time remain treat assess effectiveness policy require estimate treatment effect correspond difference outcome treat versus untreated unit develop inference procedure build computationally efficient matrix estimator treatment effect panel datum routine return confidence interval ci individual treatment effect general bilinear functional treatment effect prescribed coverage guarantee apply inferential method analyze effectiveness medicaid expansion portion affordable care act base analysis medicaid expansion lead substantial reduction uninsurance rate reduce infant mortality rate significant effect healthcare expenditure
The Parameters of Educability,"The educability model is a computational model that has been recently
proposed to describe the cognitive capability that makes humans unique among
existing biological species on Earth in being able to create advanced
civilizations. Educability is defined as a capability for acquiring and
applying knowledge. It is intended both to describe human capabilities and,
equally, as an aspirational description of what can be usefully realized by
machines. While the intention is to have a mathematically well-defined
computational model, in constructing an instance of the model there are a
number of decisions to make. We call these decisions {\it parameters}. In a
standard computer, two parameters are the memory capacity and clock rate. There
is no universally optimal choice for either one, or even for their ratio.
Similarly, in a standard machine learning system, two parameters are the
learning algorithm and the dataset used for training. Again, there are no
universally optimal choices known for either. An educable system has many more
parameters than either of these two kinds of system. This short paper discusses
some of the main parameters of educable systems, and the broader implications
of their existence.",2024-12-12 17:27:03+00:00,['Leslie G. Valiant'],educability model computational model recently propose describe cognitive capability make human unique exist biological specie earth able create advanced civilization educability define capability acquire apply knowledge intend describe human capability equally aspirational description usefully realize machine intention mathematically define computational model construct instance model number decision decision parameter standard computer parameter memory capacity clock rate universally optimal choice ratio similarly standard machine learning system parameter learn algorithm dataset training universally optimal choice know educable system parameter kind system short paper discuss main parameter educable system broad implication existence
Bayesian Optimization via Continual Variational Last Layer Training,"Gaussian Processes (GPs) are widely seen as the state-of-the-art surrogate
models for Bayesian optimization (BO) due to their ability to model uncertainty
and their performance on tasks where correlations are easily captured (such as
those defined by Euclidean metrics) and their ability to be efficiently updated
online. However, the performance of GPs depends on the choice of kernel, and
kernel selection for complex correlation structures is often difficult or must
be made bespoke. While Bayesian neural networks (BNNs) are a promising
direction for higher capacity surrogate models, they have so far seen limited
use due to poor performance on some problem types. In this paper, we propose an
approach which shows competitive performance on many problem types, including
some that BNNs typically struggle with. We build on variational Bayesian last
layers (VBLLs), and connect training of these models to exact conditioning in
GPs. We exploit this connection to develop an efficient online training
algorithm that interleaves conditioning and optimization. Our findings suggest
that VBLL networks significantly outperform GPs and other BNN architectures on
tasks with complex input correlations, and match the performance of well-tuned
GPs on established benchmark tasks.",2024-12-12 17:21:50+00:00,"['Paul Brunzema', 'Mikkel Jordahn', 'John Willes', 'Sebastian Trimpe', 'Jasper Snoek', 'James Harrison']",gaussian processes gps widely see state art surrogate model bayesian optimization bo ability model uncertainty performance task correlation easily capture define euclidean metric ability efficiently update online performance gps depend choice kernel kernel selection complex correlation structure difficult bespoke bayesian neural network bnns promising direction high capacity surrogate model far see limit use poor performance problem type paper propose approach show competitive performance problem type include bnn typically struggle build variational bayesian layer vbll connect training model exact conditioning gps exploit connection develop efficient online training algorithm interleave conditioning optimization finding suggest vbll network significantly outperform gps bnn architecture task complex input correlation match performance tune gps establish benchmark task
Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for Edge and Distributed Performance,"A Content Delivery Network (CDN) is a powerful system of distributed caching
servers that aims to accelerate content delivery, like high-definition video,
IoT applications, and ultra-low-latency services, efficiently and with fast
velocity. This has become of paramount importance in the post-pandemic era.
Challenges arise when exponential content volume growth and scalability across
different geographic locations are required. This paper investigates
data-driven evaluations of CDN algorithms in dynamic server selection for
latency reduction, bandwidth throttling for efficient resource management,
real-time Round Trip Time analysis for adaptive routing, and programmatic
network delay simulation to emulate various conditions. Key performance
metrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to
evaluate scalability and algorithmic efficiency through two experimental
setups: a constrained edge-like local system and a scalable FABRIC testbed. The
statistical validation of RTT trends, alongside CPU utilization, is presented
in the results. The optimization process reveals significant trade-offs between
scalability and resource consumption, providing actionable insights for
effectively deploying and enhancing CDN algorithms in edge and distributed
computing environments.",2024-12-12 17:20:26+00:00,"['Md Nurul Absur', 'Sourya Saha', 'Sifat Nawrin Nova', 'Kazi Fahim Ahmad Nasif', 'Md Rahat Ul Nasib']",content delivery network cdn powerful system distribute caching server aim accelerate content delivery like high definition video iot application ultra low latency service efficiently fast velocity paramount importance post pandemic era challenge arise exponential content volume growth scalability different geographic location require paper investigate data drive evaluation cdn algorithm dynamic server selection latency reduction bandwidth throttle efficient resource management real time round trip time analysis adaptive routing programmatic network delay simulation emulate condition key performance metric round trip time rtt cpu usage carefully analyze evaluate scalability algorithmic efficiency experimental setup constrained edge like local system scalable fabric testbe statistical validation rtt trend alongside cpu utilization present result optimization process reveal significant trade off scalability resource consumption provide actionable insight effectively deploy enhance cdn algorithm edge distribute computing environment
A Novel Ensemble-Based Deep Learning Model with Explainable AI for Accurate Kidney Disease Diagnosis,"Chronic Kidney Disease (CKD) represents a significant global health
challenge, characterized by the progressive decline in renal function, leading
to the accumulation of waste products and disruptions in fluid balance within
the body. Given its pervasive impact on public health, there is a pressing need
for effective diagnostic tools to enable timely intervention. Our study delves
into the application of cutting-edge transfer learning models for the early
detection of CKD. Leveraging a comprehensive and publicly available dataset, we
meticulously evaluate the performance of several state-of-the-art models,
including EfficientNetV2, InceptionNetV2, MobileNetV2, and the Vision
Transformer (ViT) technique. Remarkably, our analysis demonstrates superior
accuracy rates, surpassing the 90% threshold with MobileNetV2 and achieving
91.5% accuracy with ViT. Moreover, to enhance predictive capabilities further,
we integrate these individual methodologies through ensemble modeling,
resulting in our ensemble model exhibiting a remarkable 96% accuracy in the
early detection of CKD. This significant advancement holds immense promise for
improving clinical outcomes and underscores the critical role of machine
learning in addressing complex medical challenges.",2024-12-12 17:18:49+00:00,"['Md. Arifuzzaman', 'Iftekhar Ahmed', 'Md. Jalal Uddin Chowdhury', 'Shadman Sakib', 'Mohammad Shoaib Rahman', 'Md. Ebrahim Hossain', 'Shakib Absar']",chronic kidney disease ckd represent significant global health challenge characterize progressive decline renal function lead accumulation waste product disruption fluid balance body give pervasive impact public health press need effective diagnostic tool enable timely intervention study delve application cut edge transfer learning model early detection ckd leverage comprehensive publicly available dataset meticulously evaluate performance state art model include vision transformer vit technique remarkably analysis demonstrate superior accuracy rate surpass threshold achieve accuracy enhance predictive capability integrate individual methodology ensemble modeling result ensemble model exhibit remarkable accuracy early detection ckd significant advancement hold immense promise improve clinical outcome underscore critical role machine learning address complex medical challenge
Anharmonic long-range electron-phonon interaction: analytic formalism,"Describing electron-phonon interactions in a solid requires knowledge of the
electron-phonon matrix elements in the Hamiltonian. State-of-the-art
first-principles calculations for the electron-phonon interaction are limited
to the 1-electron-1-phonon matrix element, which is suitable for harmonic
materials. However, there is no first-principles theory for 1-electron-2-phonon
interactions, which occur in anharmonic materials with significant
electron-phonon interaction such as halide perovskites and quantum
paraelectrics. Here, we derive an analytical expression for the long-range part
of the 1-electron-2-phonon matrix element, written in terms of microscopic
quantities that can be calculated from first principles. We show that the
long-range 1-electron-2-phonon interaction is described by the derivative of
the phonon dynamical matrix with respect to an external electric field. We
calculate the quasiparticle energy of a large polaron including
1-electron-2-phonon interaction, and show that it can be written in terms of a
1-electron-2-phonon spectral function $\mathcal{T}_{\alpha \beta}(\omega)$. We
demonstrate how to calculate this spectral function for the benchmark material
LiF. The first-principles framework developed in this article is general,
paving the way for future calculations of 1-electron-2-phonon interactions from
first principles.",2024-12-12 17:17:58+00:00,"['Matthew Houtput', 'Luigi Ranalli', 'Carla Verdi', 'Serghei Klimin', 'Stefano Ragni', 'Cesare Franchini', 'Jacques Tempere']",describe electron phonon interaction solid require knowledge electron phonon matrix element hamiltonian state art principle calculation electron phonon interaction limit phonon matrix element suitable harmonic material principle theory phonon interaction occur anharmonic material significant electron phonon interaction halide perovskite quantum paraelectric derive analytical expression long range phonon matrix element write term microscopic quantity calculate principle long range phonon interaction describe derivative phonon dynamical matrix respect external electric field calculate quasiparticle energy large polaron include phonon interaction write term phonon spectral function demonstrate calculate spectral function benchmark material principle framework develop article general pave way future calculation phonon interaction principle
Neural Network Symmetrisation in Concrete Settings,"Cornish (2024) recently gave a general theory of neural network
symmetrisation in the abstract context of Markov categories. We give a
high-level overview of these results, and their concrete implications for the
symmetrisation of deterministic functions and of Markov kernels.",2024-12-12 17:16:41+00:00,['Rob Cornish'],cornish recently give general theory neural network symmetrisation abstract context markov category high level overview result concrete implication symmetrisation deterministic function markov kernel
STORM: A Spatio-Temporal Factor Model Based on Dual Vector Quantized Variational Autoencoders for Financial Trading,"In financial trading, factor models are widely used to price assets and
capture excess returns from mispricing. Recently, we have witnessed the rise of
variational autoencoder-based latent factor models, which learn latent factors
self-adaptively. While these models focus on modeling overall market
conditions, they often fail to effectively capture the temporal patterns of
individual stocks. Additionally, representing multiple factors as single values
simplifies the model but limits its ability to capture complex relationships
and dependencies. As a result, the learned factors are of low quality and lack
diversity, reducing their effectiveness and robustness across different trading
periods. To address these issues, we propose a Spatio-Temporal factOR Model
based on dual vector quantized variational autoencoders, named STORM, which
extracts features of stocks from temporal and spatial perspectives, then fuses
and aligns these features at the fine-grained and semantic level, and
represents the factors as multi-dimensional embeddings. The discrete codebooks
cluster similar factor embeddings, ensuring orthogonality and diversity, which
helps distinguish between different factors and enables factor selection in
financial trading. To show the performance of the proposed factor model, we
apply it to two downstream experiments: portfolio management on two stock
datasets and individual trading tasks on six specific stocks. The extensive
experiments demonstrate STORM's flexibility in adapting to downstream tasks and
superior performance over baseline models.",2024-12-12 17:15:49+00:00,"['Yilei Zhao', 'Wentao Zhang', 'Tingran Yang', 'Yong Jiang', 'Fei Huang', 'Wei Yang Bryan Lim']",financial trading factor model widely price asset capture excess return misprice recently witness rise variational autoencoder base latent factor model learn latent factor self adaptively model focus model overall market condition fail effectively capture temporal pattern individual stock additionally represent multiple factor single value simplify model limit ability capture complex relationship dependency result learn factor low quality lack diversity reduce effectiveness robustness different trading period address issue propose spatio temporal factor model base dual vector quantize variational autoencoder name storm extract feature stock temporal spatial perspective fuse align feature fine grain semantic level represent factor multi dimensional embedding discrete codebook cluster similar factor embedding ensure orthogonality diversity helps distinguish different factor enable factor selection financial trading performance propose factor model apply downstream experiment portfolio management stock dataset individual trading task specific stock extensive experiment demonstrate storm flexibility adapt downstream task superior performance baseline model
Distributional Reinforcement Learning based Integrated Decision Making and Control for Autonomous Surface Vehicles,"With the growing demands for Autonomous Surface Vehicles (ASVs) in recent
years, the number of ASVs being deployed for various maritime missions is
expected to increase rapidly in the near future. However, it is still
challenging for ASVs to perform sensor-based autonomous navigation in
obstacle-filled and congested waterways, where perception errors, closely
gathered vehicles and limited maneuvering space near buoys may cause
difficulties in following the Convention on the International Regulations for
Preventing Collisions at Sea (COLREGs). To address these issues, we propose a
novel Distributional Reinforcement Learning based navigation system that can
work with onboard LiDAR and odometry sensors to generate arbitrary thrust
commands in continuous action space. Comprehensive evaluations of the proposed
system in high-fidelity Gazebo simulations show its ability to decide whether
to follow COLREGs or take other beneficial actions based on the scenarios
encountered, offering superior performance in navigation safety and efficiency
compared to systems using state-of-the-art Distributional RL,
non-Distributional RL and classical methods.",2024-12-12 17:15:22+00:00,"['Xi Lin', 'Paul Szenher', 'Yewei Huang', 'Brendan Englot']",grow demand autonomous surface vehicles asvs recent year number asv deploy maritime mission expect increase rapidly near future challenge asv perform sensor base autonomous navigation obstacle fill congested waterway perception error closely gather vehicle limited maneuver space near buoy cause difficulty follow convention international regulations prevent collision sea colreg address issue propose novel distributional reinforcement learning base navigation system work onboard lidar odometry sensor generate arbitrary thrust command continuous action space comprehensive evaluation propose system high fidelity gazebo simulation ability decide follow colreg beneficial action base scenario encounter offer superior performance navigation safety efficiency compare system state art distributional rl non distributional rl classical method
OFTSR: One-Step Flow for Image Super-Resolution with Tunable Fidelity-Realism Trade-offs,"Recent advances in diffusion and flow-based generative models have
demonstrated remarkable success in image restoration tasks, achieving superior
perceptual quality compared to traditional deep learning approaches. However,
these methods either require numerous sampling steps to generate high-quality
images, resulting in significant computational overhead, or rely on model
distillation, which usually imposes a fixed fidelity-realism trade-off and thus
lacks flexibility. In this paper, we introduce OFTSR, a novel flow-based
framework for one-step image super-resolution that can produce outputs with
tunable levels of fidelity and realism. Our approach first trains a conditional
flow-based super-resolution model to serve as a teacher model. We then distill
this teacher model by applying a specialized constraint. Specifically, we force
the predictions from our one-step student model for same input to lie on the
same sampling ODE trajectory of the teacher model. This alignment ensures that
the student model's single-step predictions from initial states match the
teacher's predictions from a closer intermediate state. Through extensive
experiments on challenging datasets including FFHQ (256$\times$256), DIV2K, and
ImageNet (256$\times$256), we demonstrate that OFTSR achieves state-of-the-art
performance for one-step image super-resolution, while having the ability to
flexibly tune the fidelity-realism trade-off. Code and pre-trained models are
available at https://github.com/yuanzhi-zhu/OFTSR and
https://huggingface.co/Yuanzhi/OFTSR, respectively.",2024-12-12 17:14:58+00:00,"['Yuanzhi Zhu', 'Ruiqing Wang', 'Shilin Lu', 'Junnan Li', 'Hanshu Yan', 'Kai Zhang']",recent advance diffusion flow base generative model demonstrate remarkable success image restoration task achieve superior perceptual quality compare traditional deep learning approach method require numerous sample step generate high quality image result significant computational overhead rely model distillation usually impose fix fidelity realism trade lack flexibility paper introduce oftsr novel flow base framework step image super resolution produce output tunable level fidelity realism approach train conditional flow base super resolution model serve teacher model distill teacher model apply specialized constraint specifically force prediction step student model input lie sampling ode trajectory teacher model alignment ensure student model single step prediction initial state match teacher prediction close intermediate state extensive experiment challenge dataset include ffhq k imagenet demonstrate oftsr achieve state art performance step image super resolution have ability flexibly tune fidelity realism trade code pre train model available respectively
Constraints on Pre-Big-Bang Cosmology from Advanced LIGO and Advanced Virgo's First Three Observing Runs,"We search for the stochastic gravitational-wave background (SGWB) predicted
by pre-big-bang (PBB) cosmology using data from the first three observing runs
of Advanced LIGO and Advanced Virgo. PBB cosmology proposes an alternative to
cosmic inflation where the Universe evolves from a weak-coupling, low-curvature
state to the hot Big Bang through a high-curvature bounce phase, predicting a
distinctive SGWB spectrum. We perform a Bayesian analysis of the
cross-correlation data to constrain the model parameters characterizing the PBB
spectrum. We find no evidence for a PBB-induced SGWB, with a Bayes factor of
$0.03$ between the PBB and noise-only model, strongly favoring the noise-only
hypothesis. Our analysis establishes a lower bound $\beta \gtrsim -0.19$ at
$95\%$ confidence level, which is compatible with the theoretical requirement
$\beta \geq 0$ for a smooth bounce transition. While we do not detect a signal,
our constraints remain consistent with the basic theoretical framework of PBB
cosmology, demonstrating the potential of gravitational-wave observations to
test early Universe theories.",2024-12-12 17:12:56+00:00,"['Qin Tan', 'Zu Cheng Chen', 'You Wu', 'Lang Liu']",search stochastic gravitational wave background sgwb predict pre big bang pbb cosmology datum observing run advanced ligo advanced virgo pbb cosmology propose alternative cosmic inflation universe evolve weak coupling low curvature state hot big bang high curvature bounce phase predict distinctive sgwb spectrum perform bayesian analysis cross correlation datum constrain model parameter characterize pbb spectrum find evidence pbb induce sgwb bayes factor pbb noise model strongly favor noise hypothesis analysis establish lower bind confidence level compatible theoretical requirement smooth bounce transition detect signal constraint remain consistent basic theoretical framework pbb cosmology demonstrate potential gravitational wave observation test early universe theory
Fast and Automatic Full Waveform Inversion by Dual Augmented Lagrangian,"Full Waveform Inversion (FWI) stands as a nonlinear, high-resolution
technology for subsurface imaging via surface-recorded data. This paper
introduces an augmented Lagrangian dual formulation for FWI, rooted in the
viewpoint that Lagrange multipliers serve as fundamental unknowns for the
accurate linearization of the FWI problem. Once these multipliers are
estimated, the determination of model parameters becomes simple. Therefore,
unlike traditional primal algorithms, the proposed dual method circumvents
direct engagement with model parameters or wavefields, instead tackling the
estimation of Lagrange multipliers through a gradient ascent iteration. This
approach yields two significant advantages: i) the background model remains
fixed, requiring only one LU matrix factorization for each frequency inversion.
ii) Convergence of the algorithm can be improved by leveraging techniques like
quasi-Newton l-BFGS methods and Anderson acceleration. Numerical examples from
elastic and acoustic FWI utilizing different benchmark models are provided,
showing that the dual algorithm converges quickly and requires fewer
computations than the standard primal algorithm.",2024-12-12 17:09:34+00:00,"['Kamal Aghazade', 'Ali Gholami']",waveform inversion fwi stand nonlinear high resolution technology subsurface imaging surface record datum paper introduce augmented lagrangian dual formulation fwi root viewpoint lagrange multiplier serve fundamental unknown accurate linearization fwi problem multiplier estimate determination model parameter simple unlike traditional primal algorithm propose dual method circumvent direct engagement model parameter wavefield instead tackle estimation lagrange multiplier gradient ascent iteration approach yield significant advantage background model remains fix require lu matrix factorization frequency inversion ii convergence algorithm improve leverage technique like quasi newton l bfgs method anderson acceleration numerical example elastic acoustic fwi utilize different benchmark model provide show dual algorithm converge quickly require few computation standard primal algorithm
First-principles theory of nonlinear long-range electron-phonon interaction,"Electron-phonon interactions in solids are crucial for understanding many
interesting phenomena, such as conventional superconductivity,
temperature-dependent band-gap renormalization, and polarons. For harmonic
materials, the linear interaction of one electron with one phonon is sufficient
to quantitatively describe these properties. However, in anharmonic materials
such as quantum paraelectrics, halide perovskites, and high-pressure hydrides,
the nonlinear electron-phonon interactions may play an important role.
Currently, the only available Hamiltonians for nonlinear electron-phonon
interaction are model Hamiltonians, written in terms of phenomenological
parameters. Here, we present a microscopic theory for long-range nonlinear
electron-phonon interactions, which can be combined with first-principles
calculations. We provide a semi-analytical expression for the long-range part
of the 1-electron-2-phonon matrix element. We show that in contrast to the
long-range 1-electron-1-phonon interaction, the continuum approximation is not
sufficient and the entire phonon dispersion must be taken into account.
Additionally, we show that the quasiparticle energies can be written in terms
of a 1-electron-2-phonon spectral function. To demonstrate the method, we
calculate the 1-electron-2-phonon spectral function for LiF and KTaO$_3$ from
first principles. Our framework is a step forward toward complete
first-principles calculations of nonlinear electron-phonon interactions in
solids.",2024-12-12 17:08:18+00:00,"['Matthew Houtput', 'Luigi Ranalli', 'Carla Verdi', 'Serghei Klimin', 'Stefano Ragni', 'Jacques Tempere', 'Cesare Franchini']",electron phonon interaction solid crucial understand interesting phenomenon conventional superconductivity temperature dependent band gap renormalization polaron harmonic material linear interaction electron phonon sufficient quantitatively describe property anharmonic material quantum paraelectric halide perovskite high pressure hydride nonlinear electron phonon interaction play important role currently available hamiltonians nonlinear electron phonon interaction model hamiltonians write term phenomenological parameter present microscopic theory long range nonlinear electron phonon interaction combine principle calculation provide semi analytical expression long range phonon matrix element contrast long range phonon interaction continuum approximation sufficient entire phonon dispersion take account additionally quasiparticle energy write term phonon spectral function demonstrate method calculate phonon spectral function lif principle framework step forward complete principle calculation nonlinear electron phonon interaction solid
"On the evolutionary nature of massive B-type supergiants: a modern empirical reappraisal using data from IACOB, Gaia and TESS","Massive stars are key contributors to the chemodynamical evolution of
galaxies and the Universe. Despite their significance, discrepancies between
observational data and theoretical models of massive stars challenge our
understanding of these objects. A major uncertainty is the overdensity of
B-type supergiants (BSGs) in the Hertzsprung-Russell diagram, where models
predict the end of the main sequence phase (or TAMS). Is uncertain whether the
TAMS needs to be redefined or if the overdensity results from overlapping
populations following different evolutionary paths. Conceived as direct
descendants of O-type stars, BSGs may include stars not only evolving in the
main sequence but also returning from a post-red supergiant phase. A
representative fraction of massive stars are predicted to be products of binary
interaction, creating additional evolutionary channels. In addition, some
fundamental properties of BSGs such as the spin- and mass-loss rates are not as
well constrained as in O-type stars, having a significant impact on massive
star evolution. To overcome this situation, statistically significant
spectroscopic samples offer a unique opportunity to study the physical and
chemical properties of BSGs. Moreover, the advent of space astrometry and
photometry missions such as Gaia and TESS has brought a new era for studying
additional properties in detail. This thesis comprises the study of 1000
Galactic blue supergiants (O- and B-type) combining multi-epoch high-resolution
spectroscopic data from the IACOB project and the ESO archive with Gaia
distances and TESS photometry, becoming the largest holistic empirical study of
the physical, chemical, and pulsational properties of these objects performed
to date. All these properties gathered into a unique volume-limited sample
allowed to provide an empirical reassessment of the main properties of BSGs and
investigate their intricate nature.",2024-12-12 17:06:50+00:00,['Abel de Burgos'],massive star key contributor chemodynamical evolution galaxy universe despite significance discrepancy observational datum theoretical model massive star challenge understanding object major uncertainty overdensity b type supergiant bsg hertzsprung russell diagram model predict end main sequence phase tams uncertain tams need redefine overdensity result overlap population follow different evolutionary path conceive direct descendant o type star bsg include star evolve main sequence return post red supergiant phase representative fraction massive star predict product binary interaction create additional evolutionary channel addition fundamental property bsg mass loss rate constrain o type star have significant impact massive star evolution overcome situation statistically significant spectroscopic sample offer unique opportunity study physical chemical property bsg advent space astrometry photometry mission gaia tess bring new era study additional property detail thesis comprise study galactic blue supergiant b type combine multi epoch high resolution spectroscopic datum iacob project eso archive gaia distance tess photometry large holistic empirical study physical chemical pulsational property object perform date property gather unique volume limit sample allow provide empirical reassessment main property bsg investigate intricate nature
Finite-PINN: A Physics-Informed Neural Network Architecture for Solving Solid Mechanics Problems with General Geometries,"PINN models have demonstrated impressive capabilities in addressing fluid PDE
problems, and their potential in solid mechanics is beginning to emerge. This
study identifies two key challenges when using PINN to solve general solid
mechanics problems. These challenges become evident when comparing the
limitations of PINN with the well-established numerical methods commonly used
in solid mechanics, such as the finite element method (FEM). Specifically: a)
PINN models generate solutions over an infinite domain, which conflicts with
the finite boundaries typical of most solid structures; and b) the solution
space utilised by PINN is Euclidean, which is inadequate for addressing the
complex geometries often present in solid structures.
  This work proposes a PINN architecture used for general solid mechanics
problems, termed the Finite-PINN model. The proposed model aims to effectively
address these two challenges while preserving as much of the original
implementation of PINN as possible. The unique architecture of the Finite-PINN
model addresses these challenges by separating the approximation of stress and
displacement fields, and by transforming the solution space from the
traditional Euclidean space to a Euclidean-topological joint space. Several
case studies presented in this paper demonstrate that the Finite-PINN model
provides satisfactory results for a variety of problem types, including both
forward and inverse problems, in both 2D and 3D contexts. The developed
Finite-PINN model offers a promising tool for addressing general solid
mechanics problems, particularly those not yet well-explored in current
research.",2024-12-12 17:06:21+00:00,"['Haolin Li', 'Yuyang Miao', 'Zahra Sharif Khodaei', 'M. H. Aliabadi']",pinn model demonstrate impressive capability address fluid pde problem potential solid mechanic begin emerge study identify key challenge pinn solve general solid mechanic problem challenge evident compare limitation pinn establish numerical method commonly solid mechanic finite element method fem specifically pinn model generate solution infinite domain conflict finite boundary typical solid structure b solution space utilise pinn euclidean inadequate address complex geometry present solid structure work propose pinn architecture general solid mechanic problem term finite pinn model propose model aim effectively address challenge preserve original implementation pinn possible unique architecture finite pinn model address challenge separate approximation stress displacement field transform solution space traditional euclidean space euclidean topological joint space case study present paper demonstrate finite pinn model provide satisfactory result variety problem type include forward inverse problem contexts developed finite pinn model offer promising tool address general solid mechanic problem particularly explore current research
DumpyOS: A Data-Adaptive Multi-ary Index for Scalable Data Series Similarity Search,"Data series indexes are necessary for managing and analyzing the increasing
amounts of data series collections that are nowadays available. These indexes
support both exact and approximate similarity search, with approximate search
providing high-quality results within milliseconds, which makes it very
attractive for certain modern applications. Reducing the pre-processing (i.e.,
index building) time and improving the accuracy of search results are two major
challenges. DSTree and the iSAX index family are state-of-the-art solutions for
this problem. However, DSTree suffers from long index building times, while
iSAX suffers from low search accuracy. In this paper, we identify two problems
of the iSAX index family that adversely affect the overall performance. First,
we observe the presence of a proximity-compactness trade-off related to the
index structure design (i.e., the node fanout degree), significantly limiting
the efficiency and accuracy of the resulting index. Second, a skewed data
distribution will negatively affect the performance of iSAX. To overcome these
problems, we propose Dumpy, an index that employs a novel multi-ary data
structure with an adaptive node splitting algorithm and an efficient building
workflow. Furthermore, we devise Dumpy-Fuzzy as a variant of Dumpy which
further improves search accuracy by proper duplication of series. To fully
leverage the potential of modern hardware including multicore CPUs and Solid
State Drives (SSDs), we parallelize Dumpy to DumpyOS with sophisticated
indexing and pruning-based querying algorithms. An optimized approximate search
algorithm, DumpyOS-F which prominently improves the search accuracy without
violating the index, is also proposed.",2024-12-12 17:04:35+00:00,"['Zeyu Wang', 'Qitong Wang', 'Peng Wang', 'Themis Palpanas', 'Wei Wang']",data series index necessary manage analyze increase amount datum series collection nowadays available index support exact approximate similarity search approximate search provide high quality result millisecond make attractive certain modern application reduce pre processing index building time improve accuracy search result major challenge dstree isax index family state art solution problem dstree suffer long index building time isax suffer low search accuracy paper identify problem isax index family adversely affect overall performance observe presence proximity compactness trade relate index structure design node fanout degree significantly limit efficiency accuracy result index second skewed datum distribution negatively affect performance isax overcome problem propose dumpy index employ novel multi ary datum structure adaptive node splitting algorithm efficient building workflow furthermore devise dumpy fuzzy variant dumpy improve search accuracy proper duplication series fully leverage potential modern hardware include multicore cpu solid state drive ssds parallelize dumpy dumpyos sophisticated indexing pruning base query algorithm optimize approximate search algorithm dumpyos f prominently improve search accuracy violate index propose
Embeddings are all you need! Achieving High Performance Medical Image Classification through Training-Free Embedding Analysis,"Developing artificial intelligence (AI) and machine learning (ML) models for
medical imaging typically involves extensive training and testing on large
datasets, consuming significant computational time, energy, and resources.
There is a need for more efficient methods that can achieve comparable or
superior diagnostic performance without the associated resource burden. We
investigated the feasibility of replacing conventional training procedures with
an embedding-based approach that leverages concise and semantically meaningful
representations of medical images. Using pre-trained foundational
models-specifically, convolutional neural networks (CNN) like ResNet and
multimodal models like Contrastive Language-Image Pre-training (CLIP)-we
generated image embeddings for multi-class classification tasks. Simple linear
classifiers were then applied to these embeddings. The approach was evaluated
across diverse medical imaging modalities, including retinal images,
mammography, dermatoscopic images, and chest radiographs. Performance was
compared to benchmark models trained and tested using traditional methods. The
embedding-based models surpassed the benchmark area under the receiver
operating characteristic curve (AUC-ROC) scores by up to 87 percentage in
multi-class classification tasks across the various medical imaging modalities.
Notably, CLIP embedding models achieved the highest AUC-ROC scores,
demonstrating superior classification performance while significantly reducing
computational demands. Our study indicates that leveraging embeddings from
pre-trained foundational models can effectively replace conventional,
resource-intensive training and testing procedures in medical image analysis.
This embedding-based approach offers a more efficient alternative for image
segmentation, classification, and prediction, potentially accelerating AI
technology integration into clinical practice.",2024-12-12 16:59:37+00:00,"['Raj Hansini Khoiwal', 'Alan B. McMillan']",develop artificial intelligence ai machine learning ml model medical imaging typically involve extensive training testing large dataset consume significant computational time energy resource need efficient method achieve comparable superior diagnostic performance associate resource burden investigate feasibility replace conventional training procedure embed base approach leverage concise semantically meaningful representation medical image pre train foundational model specifically convolutional neural network cnn like resnet multimodal model like contrastive language image pre training generate image embedding multi class classification task simple linear classifier apply embedding approach evaluate diverse medical imaging modality include retinal image mammography dermatoscopic image chest radiograph performance compare benchmark model train test traditional method embed base model surpass benchmark area receiver operate characteristic curve auc roc score percentage multi class classification task medical imaging modality notably clip embed model achieve high auc roc score demonstrate superior classification performance significantly reduce computational demand study indicate leverage embedding pre train foundational model effectively replace conventional resource intensive training testing procedure medical image analysis embed base approach offer efficient alternative image segmentation classification prediction potentially accelerate ai technology integration clinical practice
Search Strategy Generation for Branch and Bound Using Genetic Programming,"Branch-and-Bound (B\&B) is an exact method in integer programming that
recursively divides the search space into a tree. During the resolution
process, determining the next subproblem to explore within the tree-known as
the search strategy-is crucial. Hand-crafted heuristics are commonly used, but
none are effective over all problem classes. Recent approaches utilizing neural
networks claim to make more intelligent decisions but are computationally
expensive. In this paper, we introduce GP2S (Genetic Programming for Search
Strategy), a novel machine learning approach that automatically generates a
B\&B search strategy heuristic, aiming to make intelligent decisions while
being computationally lightweight. We define a policy as a function that
evaluates the quality of a B\&B node by combining features from the node and
the problem; the search strategy policy is then defined by a best-first search
based on this node ranking. The policy space is explored using a genetic
programming algorithm, and the policy that achieves the best performance on a
training set is selected. We compare our approach with the standard method of
the SCIP solver, a recent graph neural network-based method, and handcrafted
heuristics. Our first evaluation includes three types of primal hard problems,
tested on instances similar to the training set and on larger instances. Our
method is at most 2\% slower than the best baseline and consistently
outperforms SCIP, achieving an average speedup of 11.3\%. Additionally, GP2S is
tested on the MIPLIB 2017 dataset, generating multiple heuristics from
different subsets of instances. It exceeds SCIP's average performance in 7 out
of 10 cases across 15 times more instances and under a time limit 15 times
longer, with some GP2S methods leading on most experiments in terms of the
number of feasible solutions or optimality gap.",2024-12-12 16:57:46+00:00,"['Gwen Maudet', 'Grégoire Danoy']",branch bound exact method integer programming recursively divide search space tree resolution process determine subproblem explore tree know search strategy crucial hand craft heuristic commonly effective problem class recent approach utilize neural network claim intelligent decision computationally expensive paper introduce genetic programming search strategy novel machine learn approach automatically generate search strategy heuristic aim intelligent decision computationally lightweight define policy function evaluate quality node combine feature node problem search strategy policy define well search base node ranking policy space explore genetic programming algorithm policy achieve good performance training set select compare approach standard method scip solver recent graph neural network base method handcraft heuristic evaluation include type primal hard problem test instance similar training set large instance method slow good baseline consistently outperform scip achieve average speedup additionally test miplib dataset generate multiple heuristic different subset instance exceed scip average performance case time instance time limit time long method lead experiment term number feasible solution optimality gap
MOS: Model Surgery for Pre-Trained Model-Based Class-Incremental Learning,"Class-Incremental Learning (CIL) requires models to continually acquire
knowledge of new classes without forgetting old ones. Despite Pre-trained
Models (PTMs) have shown excellent performance in CIL, catastrophic forgetting
still occurs as the model learns new concepts. Existing work seeks to utilize
lightweight components to adjust the PTM, while the forgetting phenomenon still
comes from {\em parameter and retrieval} levels. Specifically, iterative
updates of the model result in parameter drift, while mistakenly retrieving
irrelevant modules leads to the mismatch during inference. To this end, we
propose MOdel Surgery (MOS) to rescue the model from forgetting previous
knowledge. By training task-specific adapters, we continually adjust the PTM to
downstream tasks. To mitigate parameter-level forgetting, we present an adapter
merging approach to learn task-specific adapters, which aims to bridge the gap
between different components while reserve task-specific information. Besides,
to address retrieval-level forgetting, we introduce a training-free
self-refined adapter retrieval mechanism during inference, which leverages the
model's inherent ability for better adapter retrieval. By jointly rectifying
the model with those steps, MOS can robustly resist catastrophic forgetting in
the learning process. Extensive experiments on seven benchmark datasets
validate MOS's state-of-the-art performance. Code is available at:
https://github.com/sun-hailong/AAAI25-MOS",2024-12-12 16:57:20+00:00,"['Hai-Long Sun', 'Da-Wei Zhou', 'Hanbin Zhao', 'Le Gan', 'De-Chuan Zhan', 'Han-Jia Ye']",class incremental learning cil require model continually acquire knowledge new class forget old one despite pre train models ptms show excellent performance cil catastrophic forgetting occur model learn new concept exist work seek utilize lightweight component adjust ptm forgetting phenomenon come parameter retrieval level specifically iterative update model result parameter drift mistakenly retrieve irrelevant module lead mismatch inference end propose model surgery mos rescue model forget previous knowledge train task specific adapter continually adjust ptm downstream task mitigate parameter level forgetting present adapter merging approach learn task specific adapter aim bridge gap different component reserve task specific information address retrieval level forgetting introduce training free self refine adapter retrieval mechanism inference leverage model inherent ability well adapter retrieval jointly rectify model step mos robustly resist catastrophic forgetting learning process extensive experiment seven benchmark dataset validate mos state art performance code available
ATPrompt: Textual Prompt Learning with Embedded Attributes,"Textual-based prompt learning methods primarily employ multiple learnable
soft prompts and hard class tokens in a cascading manner as text prompt inputs,
aiming to align image and text (category) spaces for downstream tasks. However,
current training is restricted to aligning images with predefined known
categories and cannot be associated with unknown categories. In this work, we
propose utilizing universal attributes as a bridge to enhance the alignment
between images and unknown categories. Specifically, we introduce an
Attribute-embedded Textual Prompt learning method for vision-language models,
named ATPrompt. This approach expands the learning space of soft prompts from
the original one-dimensional category level into the multi-dimensional
attribute level by incorporating multiple universal attribute tokens into the
learnable soft prompts. Through this modification, we transform the text prompt
from a category-centric form to an attribute-category hybrid form. To finalize
the attributes for downstream tasks, we propose a differentiable attribute
search method that learns to identify representative and suitable attributes
from a candidate pool summarized by a large language model. As an easy-to-use
plug-in technique, ATPrompt can seamlessly replace the existing prompt format
of textual-based methods, offering general improvements at a negligible
computational cost. Extensive experiments on 11 datasets demonstrate the
effectiveness of our method.",2024-12-12 16:57:20+00:00,"['Zheng Li', 'Yibing Song', 'Penghai Zhao', 'Ming-Ming Cheng', 'Xiang Li', 'Jian Yang']",textual base prompt learn method primarily employ multiple learnable soft prompt hard class token cascade manner text prompt input aim align image text category space downstream task current training restrict align image predefine know category associate unknown category work propose utilize universal attribute bridge enhance alignment image unknown category specifically introduce attribute embed textual prompt learning method vision language model name atprompt approach expand learn space soft prompt original dimensional category level multi dimensional attribute level incorporate multiple universal attribute token learnable soft prompt modification transform text prompt category centric form attribute category hybrid form finalize attribute downstream task propose differentiable attribute search method learn identify representative suitable attribute candidate pool summarize large language model easy use plug technique atprompt seamlessly replace exist prompt format textual base method offer general improvement negligible computational cost extensive experiment dataset demonstrate effectiveness method
Learning to Adapt: Bio-Inspired Gait Strategies for Versatile Quadruped Locomotion,"Deep reinforcement learning (DRL) has revolutionised quadruped robot
locomotion, but existing control frameworks struggle to generalise beyond their
training-induced observational scope, resulting in limited adaptability. In
contrast, animals achieve exceptional adaptability through gait transition
strategies, diverse gait utilisation, and seamless adjustment to immediate
environmental demands. Inspired by these capabilities, we present a novel DRL
framework that incorporates key attributes of animal locomotion: gait
transition strategies, pseudo gait procedural memory, and adaptive motion
adjustments. This approach enables our framework to achieve unparalleled
adaptability, demonstrated through blind zero-shot deployment on complex
terrains and recovery from critically unstable states. Our findings offer
valuable insights into the biomechanics of animal locomotion, paving the way
for robust, adaptable robotic systems.",2024-12-12 16:56:01+00:00,"['Joseph Humphreys', 'Chengxu Zhou']",deep reinforcement learning drl revolutionise quadrupe robot locomotion exist control framework struggle generalise training induce observational scope result limited adaptability contrast animal achieve exceptional adaptability gait transition strategy diverse gait utilisation seamless adjustment immediate environmental demand inspire capability present novel drl framework incorporate key attribute animal locomotion gait transition strategy pseudo gait procedural memory adaptive motion adjustment approach enable framework achieve unparalleled adaptability demonstrate blind zero shot deployment complex terrain recovery critically unstable state finding offer valuable insight biomechanic animal locomotion pave way robust adaptable robotic system
Towards Robust and Fair Vision Learning in Open-World Environments,"The dissertation presents four key contributions toward fairness and
robustness in vision learning. First, to address the problem of large-scale
data requirements, the dissertation presents a novel Fairness Domain Adaptation
approach derived from two major novel research findings of Bijective Maximum
Likelihood and Fairness Adaptation Learning. Second, to enable the capability
of open-world modeling of vision learning, this dissertation presents a novel
Open-world Fairness Continual Learning Framework. The success of this research
direction is the result of two research lines, i.e., Fairness Continual
Learning and Open-world Continual Learning. Third, since visual data are often
captured from multiple camera views, robust vision learning methods should be
capable of modeling invariant features across views. To achieve this desired
goal, the research in this thesis will present a novel Geometry-based
Cross-view Adaptation framework to learn robust feature representations across
views. Finally, with the recent increase in large-scale videos and multimodal
data, understanding the feature representations and improving the robustness of
large-scale visual foundation models is critical. Therefore, this thesis will
present novel Transformer-based approaches to improve the robust feature
representations against multimodal and temporal data. Then, a novel Domain
Generalization Approach will be presented to improve the robustness of visual
foundation models. The research's theoretical analysis and experimental results
have shown the effectiveness of the proposed approaches, demonstrating their
superior performance compared to prior studies. The contributions in this
dissertation have advanced the fairness and robustness of machine vision
learning.",2024-12-12 16:50:52+00:00,['Thanh-Dat Truong'],dissertation present key contribution fairness robustness vision learning address problem large scale data requirement dissertation present novel fairness domain adaptation approach derive major novel research finding bijective maximum likelihood fairness adaptation learning second enable capability open world modeling vision learning dissertation present novel open world fairness continual learning framework success research direction result research line fairness continual learning open world continual learning visual datum capture multiple camera view robust vision learn method capable model invariant feature view achieve desire goal research thesis present novel geometry base cross view adaptation framework learn robust feature representation view finally recent increase large scale video multimodal datum understand feature representation improve robustness large scale visual foundation model critical thesis present novel transformer base approach improve robust feature representation multimodal temporal datum novel domain generalization approach present improve robustness visual foundation model research theoretical analysis experimental result show effectiveness propose approach demonstrate superior performance compare prior study contribution dissertation advance fairness robustness machine vision learning
Backreaction inclusive Schwinger effect,"We employ a self-consistent framework to study the backreaction effects of
particle creation in coupled semiclassical dynamics of a quantum complex scalar
field and a classical electric field in both Minkowski and de Sitter
spacetimes. This approach utilizes a general formalism to analyze the evolution
of Gaussian states of a quantized field, in the Schrodinger picture in the
presence of a background electric field. We numerically solve the resulting
nonlinear equations using initial data that consists of a Gaussian scalar field
state. This provides a self-consistent semiclassical evolution incorporating
the non-perturbative backreaction from particle production. We study the
time-dependent particle content, current density, and electric field, which are
defined in terms of the concept of instantaneous eigenstates, and describe how
they capture the time evolution of the quantized field modes. We then compare
the results with and without backreaction in flat and cosmological de Sitter
spacetime, finding that the backreaction significantly alters particle
production in both cases.",2024-12-12 16:47:22+00:00,"['Shagun Kaushal', 'Suprit Singh']",employ self consistent framework study backreaction effect particle creation couple semiclassical dynamic quantum complex scalar field classical electric field minkowski de sitter spacetime approach utilize general formalism analyze evolution gaussian state quantize field schrodinger picture presence background electric field numerically solve result nonlinear equation initial datum consist gaussian scalar field state provide self consistent semiclassical evolution incorporate non perturbative backreaction particle production study time dependent particle content current density electric field define term concept instantaneous eigenstate describe capture time evolution quantize field mode compare result backreaction flat cosmological de sitter spacetime find backreaction significantly alter particle production case
Probabilistic digital twins for geotechnical design and construction,"The digital twin approach has gained recognition as a promising solution to
the challenges faced by the Architecture, Engineering, Construction,
Operations, and Management (AECOM) industries. However, its broader application
across AECOM sectors remains limited. One significant obstacle is that
traditional digital twins rely on deterministic models, which require
deterministic input parameters. This limits their accuracy, as they do not
account for the substantial uncertainties inherent in AECOM projects. These
uncertainties are particularly pronounced in geotechnical design and
construction. To address this challenge, we propose a Probabilistic Digital
Twin (PDT) framework that extends traditional digital twin methodologies by
incorporating uncertainties, and is tailored to the requirements of
geotechnical design and construction. The PDT framework provides a structured
approach to integrating all sources of uncertainty, including aleatoric, data,
model, and prediction uncertainties, and propagates them throughout the entire
modeling process. To ensure that site-specific conditions are accurately
reflected as additional information is obtained, the PDT leverages Bayesian
methods for model updating. The effectiveness of the probabilistic digital twin
framework is showcased through an application to a highway foundation
construction project, demonstrating its potential to improve decision-making
and project outcomes in the face of significant uncertainties.",2024-12-12 16:37:28+00:00,"['Dafydd Cotoarbă', 'Daniel Straub', 'Ian FC Smith']",digital twin approach gain recognition promising solution challenge face architecture engineering construction operation management aecom industry broad application aecom sector remain limited significant obstacle traditional digital twin rely deterministic model require deterministic input parameter limit accuracy account substantial uncertainty inherent aecom project uncertainty particularly pronounce geotechnical design construction address challenge propose probabilistic digital twin pdt framework extend traditional digital twin methodology incorporate uncertainty tailor requirement geotechnical design construction pdt framework provide structured approach integrate source uncertainty include aleatoric datum model prediction uncertainty propagate entire modeling process ensure site specific condition accurately reflect additional information obtain pdt leverage bayesian method model updating effectiveness probabilistic digital twin framework showcase application highway foundation construction project demonstrate potential improve decision making project outcome face significant uncertainty
From Intention To Implementation: Automating Biomedical Research via LLMs,"Conventional biomedical research is increasingly labor-intensive due to the
exponential growth of scientific literature and datasets. Artificial
intelligence (AI), particularly Large Language Models (LLMs), has the potential
to revolutionize this process by automating various steps. Still, significant
challenges remain, including the need for multidisciplinary expertise,
logicality of experimental design, and performance measurements. This paper
introduces BioResearcher, the first end-to-end automated system designed to
streamline the entire biomedical research process involving dry lab
experiments. BioResearcher employs a modular multi-agent architecture,
integrating specialized agents for search, literature processing, experimental
design, and programming. By decomposing complex tasks into logically related
sub-tasks and utilizing a hierarchical learning approach, BioResearcher
effectively addresses the challenges of multidisciplinary requirements and
logical complexity. Furthermore, BioResearcher incorporates an LLM-based
reviewer for in-process quality control and introduces novel evaluation metrics
to assess the quality and automation of experimental protocols. BioResearcher
successfully achieves an average execution success rate of 63.07% across eight
previously unmet research objectives. The generated protocols averagely
outperform typical agent systems by 22.0% on five quality metrics. The system
demonstrates significant potential to reduce researchers' workloads and
accelerate biomedical discoveries, paving the way for future innovations in
automated research systems.",2024-12-12 16:35:05+00:00,"['Yi Luo', 'Linghang Shi', 'Yihao Li', 'Aobo Zhuang', 'Yeyun Gong', 'Ling Liu', 'Lin Chen']",conventional biomedical research increasingly labor intensive exponential growth scientific literature dataset artificial intelligence ai particularly large language models llms potential revolutionize process automate step significant challenge remain include need multidisciplinary expertise logicality experimental design performance measurement paper introduce bioresearcher end end automate system design streamline entire biomedical research process involve dry lab experiment bioresearcher employ modular multi agent architecture integrate specialized agent search literature processing experimental design programming decompose complex task logically relate sub task utilize hierarchical learning approach bioresearcher effectively address challenge multidisciplinary requirement logical complexity furthermore bioresearcher incorporate llm base reviewer process quality control introduce novel evaluation metric assess quality automation experimental protocol bioresearcher successfully achieve average execution success rate previously unmet research objective generate protocol averagely outperform typical agent system quality metric system demonstrate significant potential reduce researcher workload accelerate biomedical discovery pave way future innovation automate research system
Multimodal Music Generation with Explicit Bridges and Retrieval Augmentation,"Multimodal music generation aims to produce music from diverse input
modalities, including text, videos, and images. Existing methods use a common
embedding space for multimodal fusion. Despite their effectiveness in other
modalities, their application in multimodal music generation faces challenges
of data scarcity, weak cross-modal alignment, and limited controllability. This
paper addresses these issues by using explicit bridges of text and music for
multimodal alignment. We introduce a novel method named Visuals Music Bridge
(VMB). Specifically, a Multimodal Music Description Model converts visual
inputs into detailed textual descriptions to provide the text bridge; a
Dual-track Music Retrieval module that combines broad and targeted retrieval
strategies to provide the music bridge and enable user control. Finally, we
design an Explicitly Conditioned Music Generation framework to generate music
based on the two bridges. We conduct experiments on video-to-music,
image-to-music, text-to-music, and controllable music generation tasks, along
with experiments on controllability. The results demonstrate that VMB
significantly enhances music quality, modality, and customization alignment
compared to previous methods. VMB sets a new standard for interpretable and
expressive multimodal music generation with applications in various multimedia
fields. Demos and code are available at https://github.com/wbs2788/VMB.",2024-12-12 16:33:21+00:00,"['Baisen Wang', 'Le Zhuo', 'Zhaokai Wang', 'Chenxi Bao', 'Wu Chengjing', 'Xuecheng Nie', 'Jiao Dai', 'Jizhong Han', 'Yue Liao', 'Si Liu']",multimodal music generation aim produce music diverse input modality include text video image exist method use common embed space multimodal fusion despite effectiveness modality application multimodal music generation face challenge datum scarcity weak cross modal alignment limited controllability paper address issue explicit bridge text music multimodal alignment introduce novel method name visuals music bridge vmb specifically multimodal music description model convert visual input detailed textual description provide text bridge dual track music retrieval module combine broad target retrieval strategy provide music bridge enable user control finally design explicitly condition music generation framework generate music base bridge conduct experiment video music image music text music controllable music generation task experiment controllability result demonstrate vmb significantly enhance music quality modality customization alignment compare previous method vmb set new standard interpretable expressive multimodal music generation application multimedia field demos code available
A Plug-and-Play Algorithm for 3D Video Super-Resolution of Single-Photon LiDAR data,"Single-photon avalanche diodes (SPADs) are advanced sensors capable of
detecting individual photons and recording their arrival times with picosecond
resolution using time-correlated Single-Photon Counting detection techniques.
They are used in various applications, such as LiDAR, and can capture
high-speed sequences of binary single-photon images, offering great potential
for reconstructing 3D environments with high motion dynamics. To complement
single-photon data, they are often paired with conventional passive cameras,
which capture high-resolution (HR) intensity images at a lower frame rate.
However, 3D reconstruction from SPAD data faces challenges. Aggregating
multiple binary measurements improves precision and reduces noise but can cause
motion blur in dynamic scenes. Additionally, SPAD arrays often have lower
resolution than passive cameras. To address these issues, we propose a novel
computational imaging algorithm to improve the 3D reconstruction of moving
scenes from SPAD data by addressing the motion blur and increasing the native
spatial resolution. We adopt a plug-and-play approach within an optimization
scheme alternating between guided video super-resolution of the 3D scene, and
precise image realignment using optical flow. Experiments on synthetic data
show significantly improved image resolutions across various signal-to-noise
ratios and photon levels. We validate our method using real-world SPAD
measurements on three practical situations with dynamic objects. First on
fast-moving scenes in laboratory conditions at short range; second very low
resolution imaging of people with a consumer-grade SPAD sensor from
STMicroelectronics; and finally, HR imaging of people walking outdoors in
daylight at a range of 325 meters under eye-safe illumination conditions using
a short-wave infrared SPAD camera. These results demonstrate the robustness and
versatility of our approach.",2024-12-12 16:33:06+00:00,"['Alice Ruget', 'Lewis Wilson', 'Jonathan Leach', 'Rachael Tobin', 'Aongus Mccarthy', 'Gerald S. Buller', 'Steve Mclaughlin', 'Abderrahim Halimi']",single photon avalanche diode spad advanced sensor capable detect individual photon record arrival time picosecond resolution time correlate single photon counting detection technique application lidar capture high speed sequence binary single photon image offer great potential reconstruct environment high motion dynamic complement single photon datum pair conventional passive camera capture high resolution hr intensity image low frame rate reconstruction spad datum face challenge aggregate multiple binary measurement improve precision reduce noise cause motion blur dynamic scene additionally spad array low resolution passive camera address issue propose novel computational imaging algorithm improve reconstruction move scene spad datum address motion blur increase native spatial resolution adopt plug play approach optimization scheme alternating guide video super resolution scene precise image realignment optical flow experiment synthetic datum significantly improve image resolution signal noise ratio photon level validate method real world spad measurement practical situation dynamic object fast move scene laboratory condition short range second low resolution imaging people consumer grade spad sensor stmicroelectronics finally hr imaging people walk outdoors daylight range meter eye safe illumination condition short wave infrared spad camera result demonstrate robustness versatility approach
Data Efficient Prediction of excited-state properties using Quantum Neural Networks,"Understanding the properties of excited states of complex molecules is
crucial for many chemical and physical processes. Calculating these properties
is often significantly more resource-intensive than calculating their ground
state counterparts. We present a quantum machine learning model that predicts
excited-state properties from the molecular ground state for different
geometric configurations. The model comprises a symmetry-invariant quantum
neural network and a conventional neural network and is able to provide
accurate predictions with only a few training data points. The proposed
procedure is fully NISQ compatible. This is achieved by using a quantum circuit
that requires a number of parameters linearly proportional to the number of
molecular orbitals, along with a parameterized measurement observable, thereby
reducing the number of necessary measurements. We benchmark the algorithm on
three different molecules by evaluating its performance in predicting excited
state transition energies and transition dipole moments. We show that, in many
instances, the procedure is able to outperform various classical models that
rely solely on classical features.",2024-12-12 16:30:23+00:00,"['Manuel Hagelüken', 'Marco F. Huber', 'Marco Roth']",understand property excited state complex molecule crucial chemical physical process calculate property significantly resource intensive calculate ground state counterpart present quantum machine learning model predict excite state property molecular ground state different geometric configuration model comprise symmetry invariant quantum neural network conventional neural network able provide accurate prediction training datum point propose procedure fully nisq compatible achieve quantum circuit require number parameter linearly proportional number molecular orbital parameterized measurement observable reduce number necessary measurement benchmark algorithm different molecule evaluate performance predict excited state transition energy transition dipole moment instance procedure able outperform classical model rely solely classical feature
